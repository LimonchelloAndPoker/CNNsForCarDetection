{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verbesserte Automerkennung auf Bildern\n",
    "\n",
    "Dieses Notebook implementiert eine verbesserte Automerkennung auf Bildern mit dem trainierten CNN-Modell. Es verwendet einen kombinierten Ansatz mit Selective Search für Region Proposals und Multi-Scale-Erkennung, um Autos in Bildern zuverlässiger zu lokalisieren und zu markieren.\n",
    "\n",
    "## Überblick\n",
    "- Laden des trainierten CNN-Modells\n",
    "- Implementierung eines Selective Search Algorithmus für Region Proposals\n",
    "- Verbesserte Multi-Scale-Erkennung für verschiedene Objektgrößen\n",
    "- Optimierte Non-Maximum Suppression zur Entfernung überlappender Bounding Boxes\n",
    "- Anwendung auf Testbilder und Visualisierung der Ergebnisse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importieren der benötigten Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import cv2\n",
    "import os\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import time\n",
    "from skimage.feature import hog\n",
    "from skimage import exposure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorbereitung der Verzeichnisse\n",
    "\n",
    "Wir erstellen die notwendigen Verzeichnisse für Bilder und Ergebnisse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Verzeichnisse\n",
    "models_dir = '../models'\n",
    "data_dir = '../data'\n",
    "images_dir = '../images'\n",
    "results_dir = '../results'\n",
    "\n",
    "os.makedirs(images_dir, exist_ok=True)\n",
    "os.makedirs(results_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laden des trainierten Modells\n",
    "\n",
    "Wir laden das in den vorherigen Notebooks trainierte CNN-Modell zur Autoerkennung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Laden des trainierten Modells\n",
    "print(\"Laden des trainierten Modells...\")\n",
    "try:\n",
    "    model = load_model(os.path.join(models_dir, 'keras_cnn', 'car_detection_model.keras'))\n",
    "    print(\"Modell erfolgreich geladen.\")\n",
    "except:\n",
    "    print(\"Fehler beim Laden des Modells. Bitte stellen Sie sicher, dass das Modell trainiert wurde.\")\n",
    "    # In einem Notebook verwenden wir keinen exit(1), sondern werfen eine Exception\n",
    "    raise Exception(\"Modell konnte nicht geladen werden.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funktionen für die Bildverarbeitung und Objekterkennung\n",
    "\n",
    "### Laden und Vorverarbeiten von Bildern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_and_preprocess_image(image_path, target_size=(32, 32)):\n",
    "    \"\"\"\n",
    "    Lädt ein Bild und bereitet es für die Vorhersage vor.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Pfad zum Bild oder URL\n",
    "        target_size: Zielgröße für das Modell\n",
    "        \n",
    "    Returns:\n",
    "        image: Originalbild\n",
    "        processed_image: Vorverarbeitetes Bild für das Modell\n",
    "        (original_height, original_width): Originalgröße des Bildes\n",
    "    \"\"\"\n",
    "    # Überprüfen, ob es sich um eine URL handelt\n",
    "    if image_path.startswith('http'):\n",
    "        response = requests.get(image_path)\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "        image = np.array(image)\n",
    "    else:\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Speichern der Originalgröße\n",
    "    original_height, original_width = image.shape[:2]\n",
    "    \n",
    "    # Vorverarbeitung für das Modell\n",
    "    processed_image = cv2.resize(image, target_size)\n",
    "    processed_image = processed_image.astype('float32') / 255.0\n",
    "    \n",
    "    return image, processed_image, (original_height, original_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selective Search für Region Proposals\n",
    "\n",
    "Anstatt einen Sliding Window Ansatz zu verwenden, nutzen wir Selective Search, um potenzielle Regionen vorzuschlagen, in denen sich Autos befinden könnten. Dies ist effizienter und kann mehr Autos erkennen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def selective_search_regions(image, method='fast'):\n",
    "    \"\"\"\n",
    "    Verwendet Selective Search, um potenzielle Regionen für Objekte vorzuschlagen.\n",
    "    \n",
    "    Args:\n",
    "        image: Eingabebild\n",
    "        method: 'fast' oder 'quality' für Selective Search\n",
    "        \n",
    "    Returns:\n",
    "        regions: Liste der vorgeschlagenen Regionen (x, y, w, h)\n",
    "    \"\"\"\n",
    "    # Erstellen des Selective Search Segmentation Objekts\n",
    "    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "    \n",
    "    # Setzen des Basisbildes\n",
    "    ss.setBaseImage(image)\n",
    "    \n",
    "    # Auswahl der Strategie\n",
    "    if method == 'fast':\n",
    "        ss.switchToSelectiveSearchFast()\n",
    "    else:\n",
    "        ss.switchToSelectiveSearchQuality()\n",
    "    \n",
    "    # Durchführen der Selective Search\n",
    "    rects = ss.process()\n",
    "    \n",
    "    # Filtern der Regionen nach Größe\n",
    "    min_area = 500  # Minimale Fläche für eine Region\n",
    "    max_area = image.shape[0] * image.shape[1] * 0.8  # Maximale Fläche (80% des Bildes)\n",
    "    \n",
    "    filtered_regions = []\n",
    "    for x, y, w, h in rects:\n",
    "        area = w * h\n",
    "        if min_area <= area <= max_area and w/h >= 0.5 and w/h <= 2.0:  # Verhältnis von Breite zu Höhe\n",
    "            filtered_regions.append((x, y, w, h))\n",
    "    \n",
    "    # Begrenzen der Anzahl der Regionen, um die Verarbeitung zu beschleunigen\n",
    "    max_regions = 200\n",
    "    if len(filtered_regions) > max_regions:\n",
    "        # Sortieren nach Größe (absteigend) und Auswahl der größten Regionen\n",
    "        filtered_regions.sort(key=lambda r: r[2] * r[3], reverse=True)\n",
    "        filtered_regions = filtered_regions[:max_regions]\n",
    "    \n",
    "    return filtered_regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HOG-Feature Extraktion\n",
    "\n",
    "Wir verwenden HOG (Histogram of Oriented Gradients) Features, um die Erkennung von Autos zu verbessern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def extract_hog_features(image, target_size=(64, 64)):\n",
    "    \"\"\"\n",
    "    Extrahiert HOG-Features aus einem Bild.\n",
    "    \n",
    "    Args:\n",
    "        image: Eingabebild\n",
    "        target_size: Zielgröße für die HOG-Feature-Extraktion\n",
    "        \n",
    "    Returns:\n",
    "        hog_image: Visualisierung der HOG-Features\n",
    "    \"\"\"\n",
    "    # Konvertieren in Graustufen\n",
    "    if len(image.shape) == 3:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    else:\n",
    "        gray = image\n",
    "    \n",
    "    # Skalieren auf die Zielgröße\n",
    "    resized = cv2.resize(gray, target_size)\n",
    "    \n",
    "    # HOG-Features extrahieren\n",
    "    features, hog_image = hog(resized, orientations=9, pixels_per_cell=(8, 8),\n",
    "                             cells_per_block=(2, 2), visualize=True, feature_vector=True)\n",
    "    \n",
    "    # Normalisieren für die Visualisierung\n",
    "    hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
    "    \n",
    "    return hog_image_rescaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verbesserte Objekterkennung mit Region Proposals\n",
    "\n",
    "Diese Funktion kombiniert Selective Search für Region Proposals mit dem CNN-Modell zur Klassifikation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def detect_cars_with_region_proposals(image, model, confidence_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Erkennt Autos in einem Bild mit Selective Search für Region Proposals.\n",
    "    \n",
    "    Args:\n",
    "        image: Eingabebild\n",
    "        model: Trainiertes Modell\n",
    "        confidence_threshold: Schwellenwert für die Konfidenz\n",
    "        \n",
    "    Returns:\n",
    "        detections: Liste der erkannten Autos (x, y, w, h, confidence)\n",
    "    \"\"\"\n",
    "    # Regionen mit Selective Search vorschlagen\n",
    "    regions = selective_search_regions(image)\n",
    "    \n",
    "    detections = []\n",
    "    \n",
    "    for x, y, w, h in regions:\n",
    "        # Extrahieren der Region\n",
    "        region = image[y:y+h, x:x+w]\n",
    "        \n",
    "        # Vorverarbeitung der Region\n",
    "        try:\n",
    "            region_resized = cv2.resize(region, (32, 32))\n",
    "            region_normalized = region_resized.astype('float32') / 255.0\n",
    "            region_batch = np.expand_dims(region_normalized, axis=0)\n",
    "            \n",
    "            # Vorhersage\n",
    "            prediction = model.predict(region_batch, verbose=0)[0][0]\n",
    "            \n",
    "            # Wenn die Konfidenz über dem Schwellenwert liegt, speichern wir die Erkennung\n",
    "            if prediction > confidence_threshold:\n",
    "                detections.append((x, y, w, h, prediction))\n",
    "        except Exception as e:\n",
    "            # Ignorieren von Regionen, die nicht verarbeitet werden können\n",
    "            continue\n",
    "    \n",
    "    return detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verbesserte Multi-Scale-Erkennung\n",
    "\n",
    "Diese Funktion kombiniert Region Proposals mit Multi-Scale-Erkennung für eine verbesserte Automerkennung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def detect_cars_multi_scale_improved(image, model, scales=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0, 2.5], \n",
    "                                    confidence_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Erkennt Autos in einem Bild mit verbesserter Multi-Scale-Erkennung.\n",
    "    \n",
    "    Args:\n",
    "        image: Eingabebild\n",
    "        model: Trainiertes Modell\n",
    "        scales: Liste der Skalierungsfaktoren\n",
    "        confidence_threshold: Schwellenwert für die Konfidenz\n",
    "        \n",
    "    Returns:\n",
    "        detections: Liste der erkannten Autos (x, y, w, h, confidence)\n",
    "    \"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    detections = []\n",
    "    \n",
    "    # Erkennung mit Region Proposals auf dem Originalbild\n",
    "    detections.extend(detect_cars_with_region_proposals(image, model, confidence_threshold))\n",
    "    \n",
    "    # Multi-Scale-Erkennung\n",
    "    for scale in scales:\n",
    "        # Skalieren des Bildes\n",
    "        scaled_height = int(height * scale)\n",
    "        scaled_width = int(width * scale)\n",
    "        scaled_image = cv2.resize(image, (scaled_width, scaled_height))\n",
    "        \n",
    "        # Erkennen von Autos im skalierten Bild mit Region Proposals\n",
    "        scaled_detections = detect_cars_with_region_proposals(scaled_image, model, confidence_threshold)\n",
    "        \n",
    "        # Anpassen der Koordinaten an die Originalgröße\n",
    "        for (x, y, w, h, conf) in scaled_detections:\n",
    "            x_orig = int(x / scale)\n",
    "            y_orig = int(y / scale)\n",
    "            w_orig = int(w / scale)\n",
    "            h_orig = int(h / scale)\n",
    "            detections.append((x_orig, y_orig, w_orig, h_orig, conf))\n",
    "    \n",
    "    return detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verbesserte Non-Maximum Suppression\n",
    "\n",
    "Diese Funktion verwendet einen verbesserten Algorithmus für Non-Maximum Suppression, um überlappende Bounding Boxes zu entfernen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def improved_non_max_suppression(boxes, overlap_threshold=0.3, score_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Führt eine verbesserte Non-Maximum Suppression durch, um überlappende Bounding Boxes zu entfernen.\n",
    "    \n",
    "    Args:\n",
    "        boxes: Liste der Bounding Boxes (x, y, w, h, confidence)\n",
    "        overlap_threshold: Schwellenwert für die Überlappung\n",
    "        score_threshold: Minimaler Konfidenzwert\n",
    "        \n",
    "    Returns:\n",
    "        picked: Liste der ausgewählten Bounding Boxes\n",
    "    \"\"\"\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Filtern nach Konfidenz\n",
    "    boxes = [box for box in boxes if box[4] >= score_threshold]\n",
    "    \n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Konvertieren der Bounding Boxes in das Format (x1, y1, x2, y2, conf)\n",
    "    boxes_array = np.array([(x, y, x + w, y + h, conf) for x, y, w, h, conf in boxes])\n",
    "    \n",
    "    # Sortieren der Bounding Boxes nach Konfidenz (absteigend)\n",
    "    boxes_array = boxes_array[np.argsort(boxes_array[:, 4])[::-1]]\n",
    "    \n",
    "    picked = []\n",
    "    \n",
    "    while len(boxes_array) > 0:\n",
    "        # Die Box mit der höchsten Konfidenz auswählen\n",
    "        current_box = boxes_array[0]\n",
    "        picked.append(current_box)\n",
    "        \n",
    "        # Berechnen der Überlappung mit den verbleibenden Boxen\n",
    "        remaining_boxes = boxes_array[1:]\n",
    "        \n",
    "        if len(remaining_boxes) == 0:\n",
    "            break\n",
    "        \n",
    "        # Berechnen der Koordinaten der Überlappung\n",
    "        xx1 = np.maximum(current_box[0], remaining_boxes[:, 0])\n",
    "        yy1 = np.maximum(current_box[1], remaining_boxes[:, 1])\n",
    "        xx2 = np.minimum(current_box[2], remaining_boxes[:, 2])\n",
    "        yy2 = np.minimum(current_box[3], remaining_boxes[:, 3])\n",
    "        \n",
    "        # Berechnen der Breite und Höhe der Überlappung\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    "        \n",
    "        # Berechnen des Überlappungsverhältnisses (IoU - Intersection over Union)\n",
    "        intersection = w * h\n",
    "        area1 = (current_box[2] - current_box[0] + 1) * (current_box[3] - current_box[1] + 1)\n",
    "        area2 = (remaining_boxes[:, 2] - remaining_boxes[:, 0] + 1) * (remaining_boxes[:, 3] - remaining_boxes[:, 1] + 1)\n",
    "        union = area1 + area2 - intersection\n",
    "        iou = intersection / union\n",
    "        \n",
    "        # Entfernen der Boxen mit einer Überlappung über dem Schwellenwert\n",
    "        boxes_array = remaining_boxes[iou < overlap_threshold]\n",
    "    \n",
    "    # Konvertieren zurück in das Format (x, y, w, h, confidence)\n",
    "    picked = [(box[0], box[1], box[2] - box[0], box[3] - box[1], box[4]) for box in picked]\n",
    "    \n",
    "    return picked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zeichnen der Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def draw_boxes(image, boxes):\n",
    "    \"\"\"\n",
    "    Zeichnet Bounding Boxes auf ein Bild.\n",
    "    \n",
    "    Args:\n",
    "        image: Eingabebild\n",
    "        boxes: Liste der Bounding Boxes (x, y, w, h, confidence)\n",
    "        \n",
    "    Returns:\n",
    "        result: Bild mit Bounding Boxes\n",
    "    \"\"\"\n",
    "    result = image.copy()\n",
    "    \n",
    "    for (x, y, w, h, conf) in boxes:\n",
    "        # Convert coordinates to integers to avoid TypeError\n",
    "        x, y, w, h = int(x), int(y), int(w), int(h)\n",
    "        \n",
    "        # Zeichnen der Bounding Box\n",
    "        cv2.rectangle(result, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        \n",
    "        # Zeichnen des Konfidenzwerts\n",
    "        label = f\"Car: {conf:.2f}\"\n",
    "        cv2.putText(result, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hauptfunktion zur verbesserten Erkennung von Autos in Bildern\n",
    "\n",
    "Diese Funktion kombiniert alle vorherigen Funktionen, um Autos in einem Bild zu erkennen und zu markieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def detect_and_draw_cars_improved(image_path, model, output_path):\n",
    "    \"\"\"\n",
    "    Erkennt Autos in einem Bild mit verbesserter Methode und zeichnet Bounding Boxes.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Pfad zum Bild oder URL\n",
    "        model: Trainiertes Modell\n",
    "        output_path: Pfad zum Ausgabebild\n",
    "        \n",
    "    Returns:\n",
    "        boxes: Liste der erkannten Autos (x, y, w, h, confidence)\n",
    "    \"\"\"\n",
    "    # Laden und Vorverarbeiten des Bildes\n",
    "    image, processed_image, (original_height, original_width) = load_and_preprocess_image(image_path)\n",
    "    \n",
    "    # Zeitmessung starten\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Erkennen von Autos im Bild mit verbesserter Multi-Scale-Erkennung\n",
    "    boxes = detect_cars_multi_scale_improved(image, model, confidence_threshold=0.6)\n",
    "    \n",
    "    # Zusammenführen überlappender Bounding Boxes mit verbesserter NMS\n",
    "    boxes = improved_non_max_suppression(boxes, overlap_threshold=0.3, score_threshold=0.6)\n",
    "    \n",
    "    # Zeitmessung beenden\n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    \n",
    "    # Zeichnen der Bounding Boxes\n",
    "    result = draw_boxes(image, boxes)\n",
    "    \n",
    "    # Hinzufügen von Informationen zum Bild\n",
    "    info_text = f\"Erkannte Autos: {len(boxes)} | Verarbeitungszeit: {processing_time:.2f}s\"\n",
    "    cv2.putText(result, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "    \n",
    "    # Speichern des Ergebnisses\n",
    "    result_rgb = cv2.cvtColor(result, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(output_path, result_rgb)\n",
    "    \n",
    "    # Erstellen einzelner Bilder für jedes erkannte Auto\n",
    "    for i, (x, y, w, h, conf) in enumerate(boxes):\n",
    "        # Convert coordinates to integers to avoid TypeError\n",
    "        x, y, w, h = int(x), int(y), int(w), int(h)\n",
    "        car_image = image[y:y+h, x:x+w]\n",
    "        car_image_with_box = car_image.copy()\n",
    "        cv2.rectangle(car_image_with_box, (0, 0), (w, h), (0, 255, 0), 2)\n",
    "        \n",
    "        # Speichern des Bildes\n",
    "        car_output_path = output_path.replace('.jpg', f'_car_{i+1}.jpg')\n",
    "        car_image_rgb = cv2.cvtColor(car_image_with_box, cv2.COLOR_RGB2BGR)\n",
    "        cv2.imwrite(car_output_path, car_image_rgb)\n",
    "    \n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anwendung auf Testbilder\n",
    "\n",
    "### Testen der verbesserten Automerkennung auf den Bildern aus dem Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"Testen der verbesserten Automerkennung auf den Bildern aus dem Repository...\")\n",
    "\n",
    "# Bilder aus dem Repository\n",
    "repo_images = [\n",
    "    os.path.join(images_dir, 'bild1.jpg'),\n",
    "    os.path.join(images_dir, 'bild2.jpg'),\n",
    "    os.path.join(images_dir, 'bild3.jpg')\n",
    "]\n",
    "\n",
    "for i, image_path in enumerate(repo_images):\n",
    "    # Erkennen von Autos im Bild\n",
    "    output_path = os.path.join(results_dir, f'bild{i+1}_result_improved.jpg')\n",
    "    boxes = detect_and_draw_cars_improved(image_path, model, output_path)\n",
    "    \n",
    "    print(f\"Bild {i+1}: {len(boxes)} Autos erkannt\")\n",
    "    \n",
    "    # Anzeigen des Ergebnisses im Notebook\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(plt.imread(output_path))\n",
    "    plt.title(f\"Bild {i+1}: {len(boxes)} Autos erkannt\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testen der verbesserten Automerkennung auf zusätzlichen Bildern aus dem Internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"Testen der verbesserten Automerkennung auf zusätzlichen Bildern aus dem Internet...\")\n",
    "\n",
    "# Suchen nach Bildern mit mehreren Autos\n",
    "additional_images = [\n",
    "    \"https://cdn.pixabay.com/photo/2017/11/23/04/13/traffic-jam-2972156_1280.jpg\",\n",
    "    \"https://cdn.pixabay.com/photo/2017/08/01/09/34/car-2563902_1280.jpg\",\n",
    "    \"https://cdn.pixabay.com/photo/2017/08/06/12/08/car-2592150_1280.jpg\"\n",
    "]\n",
    "\n",
    "for i, image_url in enumerate(additional_images):\n",
    "    # Speichern des Bildes\n",
    "    image_path = os.path.join(images_dir, f'additional_image_{i+1}.jpg')\n",
    "    \n",
    "    # Herunterladen des Bildes, wenn es noch nicht existiert\n",
    "    if not os.path.exists(image_path):\n",
    "        response = requests.get(image_url)\n",
    "        with open(image_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "    \n",
    "    # Erkennen von Autos im Bild\n",
    "    output_path = os.path.join(results_dir, f'additional_image_{i+1}_result_improved.jpg')\n",
    "    boxes = detect_and_draw_cars_improved(image_path, model, output_path)\n",
    "    \n",
    "    print(f\"Zusätzliches Bild {i+1}: {len(boxes)} Autos erkannt\")\n",
    "    \n",
    "    # Anzeigen des Ergebnisses im Notebook\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(plt.imread(output_path))\n",
    "    plt.title(f\"Zusätzliches Bild {i+1}: {len(boxes)} Autos erkannt\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vergleich mit der ursprünglichen Methode\n",
    "\n",
    "Hier vergleichen wir die Ergebnisse der verbesserten Methode mit der ursprünglichen Sliding-Window-Methode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def detect_cars(image, model, window_size=(64, 64), stride=32, confidence_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Erkennt Autos in einem Bild mit Sliding Window.\n",
    "    \n",
    "    Args:\n",
    "        image: Eingabebild\n",
    "        model: Trainiertes Modell\n",
    "        window_size: Größe des Sliding Windows\n",
    "        stride: Schrittweite des Sliding Windows\n",
    "        confidence_threshold: Schwellenwert für die Konfidenz\n",
    "        \n",
    "    Returns:\n",
    "        detections: Liste der erkannten Autos (x, y, w, h, confidence)\n",
    "    \"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    detections = []\n",
    "    \n",
    "    for y in range(0, height - window_size[1], stride):\n",
    "        for x in range(0, width - window_size[0], stride):\n",
    "            # Extrahieren des Fensters\n",
    "            window = image[y:y + window_size[1], x:x + window_size[0]]\n",
    "            \n",
    "            # Vorverarbeitung des Fensters\n",
    "            window = cv2.resize(window, (32, 32))\n",
    "            window = window.astype('float32') / 255.0\n",
    "            window = np.expand_dims(window, axis=0)\n",
    "            \n",
    "            # Vorhersage\n",
    "            prediction = model.predict(window, verbose=0)[0][0]\n",
    "            \n",
    "            # Wenn die Konfidenz über dem Schwellenwert liegt, speichern wir die Erkennung\n",
    "            if prediction > confidence_threshold:\n",
    "                detections.append((x, y, window_size[0], window_size[1], prediction))\n",
    "    \n",
    "    return detections\n",
    "\n",
    "def detect_cars_multi_scale(image, model, scales=[0.5, 0.75, 1.0, 1.25, 1.5, 2.0, 2.5], \n",
    "                           window_size=(64, 64), stride=32, confidence_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Erkennt Autos in einem Bild mit Multi-Scale Sliding Window.\n",
    "    \n",
    "    Args:\n",
    "        image: Eingabebild\n",
    "        model: Trainiertes Modell\n",
    "        scales: Liste der Skalierungsfaktoren\n",
    "        window_size: Größe des Sliding Windows\n",
    "        stride: Schrittweite des Sliding Windows\n",
    "        confidence_threshold: Schwellenwert für die Konfidenz\n",
    "        \n",
    "    Returns:\n",
    "        detections: Liste der erkannten Autos (x, y, w, h, confidence)\n",
    "    \"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    detections = []\n",
    "    \n",
    "    for scale in scales:\n",
    "        # Skalieren des Bildes\n",
    "        scaled_height = int(height * scale)\n",
    "        scaled_width = int(width * scale)\n",
    "        scaled_image = cv2.resize(image, (scaled_width, scaled_height))\n",
    "        \n",
    "        # Erkennen von Autos im skalierten Bild\n",
    "        scaled_detections = detect_cars(scaled_image, model, window_size, stride, confidence_threshold)\n",
    "        \n",
    "        # Anpassen der Koordinaten an die Originalgröße\n",
    "        for (x, y, w, h, conf) in scaled_detections:\n",
    "            x_orig = int(x / scale)\n",
    "            y_orig = int(y / scale)\n",
    "            w_orig = int(w / scale)\n",
    "            h_orig = int(h / scale)\n",
    "            detections.append((x_orig, y_orig, w_orig, h_orig, conf))\n",
    "    \n",
    "    return detections\n",
    "\n",
    "def non_max_suppression(boxes, overlap_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Führt Non-Maximum Suppression durch, um überlappende Bounding Boxes zu entfernen.\n",
    "    \n",
    "    Args:\n",
    "        boxes: Liste der Bounding Boxes (x, y, w, h, confidence)\n",
    "        overlap_threshold: Schwellenwert für die Überlappung\n",
    "        \n",
    "    Returns:\n",
    "        picked: Liste der ausgewählten Bounding Boxes\n",
    "    \"\"\"\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Konvertieren der Bounding Boxes in das Format (x1, y1, x2, y2)\n",
    "    boxes_array = np.array([(x, y, x + w, y + h, conf) for x, y, w, h, conf in boxes])\n",
    "    \n",
    "    # Sortieren der Bounding Boxes nach Konfidenz (absteigend)\n",
    "    boxes_array = boxes_array[np.argsort(boxes_array[:, 4])[::-1]]\n",
    "    \n",
    "    picked = []\n",
    "    \n",
    "    while len(boxes_array) > 0:\n",
    "        # Die Box mit der höchsten Konfidenz auswählen\n",
    "        current_box = boxes_array[0]\n",
    "        picked.append(current_box)\n",
    "        \n",
    "        # Berechnen der Überlappung mit den verbleibenden Boxen\n",
    "        remaining_boxes = boxes_array[1:]\n",
    "        \n",
    "        if len(remaining_boxes) == 0:\n",
    "            break\n",
    "        \n",
    "        # Berechnen der Koordinaten der Überlappung\n",
    "        xx1 = np.maximum(current_box[0], remaining_boxes[:, 0])\n",
    "        yy1 = np.maximum(current_box[1], remaining_boxes[:, 1])\n",
    "        xx2 = np.minimum(current_box[2], remaining_boxes[:, 2])\n",
    "        yy2 = np.minimum(current_box[3], remaining_boxes[:, 3])\n",
    "        \n",
    "        # Berechnen der Breite und Höhe der Überlappung\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    "        \n",
    "        # Berechnen des Überlappungsverhältnisses\n",
    "        overlap = (w * h) / ((remaining_boxes[:, 2] - remaining_boxes[:, 0] + 1) * \n",
    "                             (remaining_boxes[:, 3] - remaining_boxes[:, 1] + 1))\n",
    "        \n",
    "        # Entfernen der Boxen mit einer Überlappung über dem Schwellenwert\n",
    "        boxes_array = remaining_boxes[overlap < overlap_threshold]\n",
    "    \n",
    "    # Konvertieren zurück in das Format (x, y, w, h, confidence)\n",
    "    picked = [(box[0], box[1], box[2] - box[0], box[3] - box[1], box[4]) for box in picked]\n",
    "    \n",
    "    return picked\n",
    "\n",
    "def detect_and_draw_cars_original(image_path, model, output_path, multi_scale=True):\n",
    "    \"\"\"\n",
    "    Erkennt Autos in einem Bild und zeichnet Bounding Boxes mit der ursprünglichen Methode.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Pfad zum Bild oder URL\n",
    "        model: Trainiertes Modell\n",
    "        output_path: Pfad zum Ausgabebild\n",
    "        multi_scale: Ob Multi-Scale Sliding Window verwendet werden soll\n",
    "        \n",
    "    Returns:\n",
    "        boxes: Liste der erkannten Autos (x, y, w, h, confidence)\n",
    "    \"\"\"\n",
    "    # Laden und Vorverarbeiten des Bildes\n",
    "    image, processed_image, (original_height, original_width) = load_and_preprocess_image(image_path)\n",
    "    \n",
    "    # Zeitmessung starten\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Erkennen von Autos im Bild\n",
    "    if multi_scale:\n",
    "        boxes = detect_cars_multi_scale(image, model)\n",
    "    else:\n",
    "        boxes = detect_cars(image, model)\n",
    "    \n",
    "    # Zusammenführen überlappender Bounding Boxes\n",
    "    boxes = non_max_suppression(boxes)\n",
    "    \n",
    "    # Zeitmessung beenden\n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    \n",
    "    # Zeichnen der Bounding Boxes\n",
    "    result = draw_boxes(image, boxes)\n",
    "    \n",
    "    # Hinzufügen von Informationen zum Bild\n",
    "    info_text = f\"Erkannte Autos: {len(boxes)} | Verarbeitungszeit: {processing_time:.2f}s\"\n",
    "    cv2.putText(result, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "    \n",
    "    # Speichern des Ergebnisses\n",
    "    result_rgb = cv2.cvtColor(result, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(output_path, result_rgb)\n",
    "    \n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"Vergleich der verbesserten Methode mit der ursprünglichen Methode...\")\n",
    "\n",
    "# Bilder aus dem Repository für den Vergleich\n",
    "for i, image_path in enumerate(repo_images):\n",
    "    # Erkennen von Autos im Bild mit der ursprünglichen Methode\n",
    "    output_path_original = os.path.join(results_dir, f'bild{i+1}_result_original.jpg')\n",
    "    boxes_original = detect_and_draw_cars_original(image_path, model, output_path_original)\n",
    "    \n",
    "    # Erkennen von Autos im Bild mit der verbesserten Methode\n",
    "    output_path_improved = os.path.join(results_dir, f'bild{i+1}_result_improved.jpg')\n",
    "    \n",
    "    # Anzeigen des Vergleichs im Notebook\n",
    "    plt.figure(figsize=(18, 8))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(plt.imread(output_path_original))\n",
    "    plt.title(f\"Original: {len(boxes_original)} Autos erkannt\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(plt.imread(output_path_improved))\n",
    "    plt.title(f\"Verbessert: {len(boxes)} Autos erkannt\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zusammenfassung\n",
    "\n",
    "In diesem Notebook haben wir:\n",
    "1. Ein trainiertes CNN-Modell zur Autoerkennung geladen\n",
    "2. Einen verbesserten Ansatz zur Automerkennung implementiert, der Selective Search für Region Proposals verwendet\n",
    "3. Eine verbesserte Multi-Scale-Erkennung implementiert, um Autos unterschiedlicher Größen zuverlässiger zu erkennen\n",
    "4. Eine verbesserte Non-Maximum Suppression angewendet, um überlappende Bounding Boxes zu entfernen\n",
    "5. Die Erkennung auf verschiedenen Testbildern angewendet und die Ergebnisse visualisiert\n",
    "6. Die Ergebnisse der verbesserten Methode mit der ursprünglichen Sliding-Window-Methode verglichen\n",
    "\n",
    "Der implementierte verbesserte Ansatz ermöglicht eine zuverlässigere Erkennung von Autos in Bildern unterschiedlicher Größen und Perspektiven. Die Verwendung von Selective Search für Region Proposals anstelle des Sliding-Window-Ansatzes führt zu einer besseren Erkennung von Autos, insbesondere in komplexen Szenen mit mehreren Autos. Die verbesserte Multi-Scale-Erkennung und Non-Maximum Suppression erhöhen die Robustheit und Genauigkeit der Erkennung weiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"Verbesserte Automerkennung auf Bildern abgeschlossen. Die Ergebnisse wurden im Verzeichnis 'results' gespeichert.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
