{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verbesserte Automerkennung auf Bildern\n",
    "\n",
    "Dieses Notebook implementiert eine verbesserte Automerkennung auf Bildern mit dem trainierten CNN-Modell. Es verwendet einen kombinierten Ansatz mit Selective Search für Region Proposals und Multi-Scale-Erkennung, um Autos in Bildern zuverlässiger zu lokalisieren und zu markieren.\n",
    "\n",
    "## Überblick\n",
    "- Laden des trainierten CNN-Modells\n",
    "- Implementierung eines Selective Search Algorithmus für Region Proposals\n",
    "- Verbesserte Multi-Scale-Erkennung für verschiedene Objektgrößen\n",
    "- Optimierte Non-Maximum Suppression zur Entfernung überlappender Bounding Boxes\n",
    "- Anwendung auf Testbilder und Visualisierung der Ergebnisse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importieren der benötigten Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import cv2\n",
    "import os\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import time\n",
    "from skimage.feature import hog\n",
    "from skimage import exposure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorbereitung der Verzeichnisse\n",
    "\n",
    "Wir erstellen die notwendigen Verzeichnisse für Bilder und Ergebnisse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verzeichnisse\n",
    "models_dir = '../models'\n",
    "data_dir = '../data'\n",
    "images_dir = '../images'\n",
    "results_dir = '../results'\n",
    "\n",
    "os.makedirs(images_dir, exist_ok=True)\n",
    "os.makedirs(results_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laden des trainierten Modells\n",
    "\n",
    "Wir laden das in den vorherigen Notebooks trainierte CNN-Modell zur Autoerkennung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laden des trainierten Modells...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-21 19:08:53.389131: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modell erfolgreich geladen.\n"
     ]
    }
   ],
   "source": [
    "# Laden des trainierten Modells\n",
    "print(\"Laden des trainierten Modells...\")\n",
    "try:\n",
    "    model = load_model(os.path.join(models_dir, 'keras_cnn', 'car_detection_model.keras'))\n",
    "    print(\"Modell erfolgreich geladen.\")\n",
    "except:\n",
    "    print(\"Fehler beim Laden des Modells. Bitte stellen Sie sicher, dass das Modell trainiert wurde.\")\n",
    "    # In einem Notebook verwenden wir keinen exit(1), sondern werfen eine Exception\n",
    "    raise Exception(\"Modell konnte nicht geladen werden.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funktionen für die Bildverarbeitung und Objekterkennung\n",
    "\n",
    "### Laden und Vorverarbeiten von Bildern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(image_path, target_size=(32, 32)):\n",
    "    \"\"\"\n",
    "    Lädt ein Bild und bereitet es für die Vorhersage vor.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Pfad zum Bild oder URL\n",
    "        target_size: Zielgröße für das Modell\n",
    "        \n",
    "    Returns:\n",
    "        image: Originalbild\n",
    "        processed_image: Vorverarbeitetes Bild für das Modell\n",
    "        (original_height, original_width): Originalgröße des Bildes\n",
    "    \"\"\"\n",
    "    # Überprüfen, ob es sich um eine URL handelt\n",
    "    if image_path.startswith('http'):\n",
    "        response = requests.get(image_path)\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "        image = np.array(image)\n",
    "    else:\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Speichern der Originalgröße\n",
    "    original_height, original_width = image.shape[:2]\n",
    "    \n",
    "    # Vorverarbeitung für das Modell\n",
    "    processed_image = cv2.resize(image, target_size)\n",
    "    processed_image = processed_image.astype('float32') / 255.0\n",
    "    \n",
    "    return image, processed_image, (original_height, original_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selective Search für Region Proposals\n",
    "\n",
    "Anstatt einen Sliding Window Ansatz zu verwenden, nutzen wir Selective Search, um potenzielle Regionen vorzuschlagen, in denen sich Autos befinden könnten. Dies ist effizienter und kann mehr Autos erkennen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_search_regions(image, method='fast'):\n",
    "    \"\"\"\n",
    "    Verwendet Selective Search, um potenzielle Regionen für Objekte vorzuschlagen.\n",
    "    Falls cv2.ximgproc nicht verfügbar ist, wird eine alternative Implementierung verwendet.\n",
    "    \n",
    "    Args:\n",
    "        image: Eingabebild\n",
    "        method: 'fast' oder 'quality' für Selective Search\n",
    "        \n",
    "    Returns:\n",
    "        regions: Liste der vorgeschlagenen Regionen (x, y, w, h)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Versuchen, Selective Search zu verwenden\n",
    "        ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "        ss.setBaseImage(image)\n",
    "        \n",
    "        if method == 'fast':\n",
    "            ss.switchToSelectiveSearchFast()\n",
    "        else:\n",
    "            ss.switchToSelectiveSearchQuality()\n",
    "        \n",
    "        rects = ss.process()\n",
    "    except AttributeError:\n",
    "        # Fallback: Einfache Regionen basierend auf Bildunterteilung\n",
    "        print(\"cv2.ximgproc nicht verfügbar, verwende alternative Methode...\")\n",
    "        height, width = image.shape[:2]\n",
    "        rects = []\n",
    "        \n",
    "        # Verschiedene Fenstergrößen\n",
    "        window_sizes = [(64, 64), (96, 96), (128, 128), (196, 196), (256, 256)]\n",
    "        strides = [32, 48, 64, 98, 128]\n",
    "        \n",
    "        for window_size, stride in zip(window_sizes, strides):\n",
    "            for y in range(0, height - window_size[1], stride):\n",
    "                for x in range(0, width - window_size[0], stride):\n",
    "                    rects.append((x, y, window_size[0], window_size[1]))\n",
    "    \n",
    "    # Filtern der Regionen nach Größe\n",
    "    min_area = 500  # Minimale Fläche für eine Region\n",
    "    max_area = image.shape[0] * image.shape[1] * 0.8  # Maximale Fläche (80% des Bildes)\n",
    "    \n",
    "    filtered_regions = []\n",
    "    for x, y, w, h in rects:\n",
    "        area = w * h\n",
    "        if min_area <= area <= max_area and w/h >= 0.5 and w/h <= 2.0:  # Verhältnis von Breite zu Höhe\n",
    "            filtered_regions.append((x, y, w, h))\n",
    "    \n",
    "    # Begrenzen der Anzahl der Regionen, um die Verarbeitung zu beschleunigen\n",
    "    max_regions = 200\n",
    "    if len(filtered_regions) > max_regions:\n",
    "        # Sortieren nach Größe (absteigend) und Auswahl der größten Regionen\n",
    "        filtered_regions.sort(key=lambda r: r[2] * r[3], reverse=True)\n",
    "        filtered_regions = filtered_regions[:max_regions]\n",
    "    \n",
    "    return filtered_regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HOG-Feature Extraktion\n",
    "\n",
    "Wir verwenden HOG (Histogram of Oriented Gradients) Features, um die Erkennung von Autos zu verbessern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hog_features(image, target_size=(64, 64)):\n",
    "    \"\"\"\n",
    "    Extrahiert HOG-Features aus einem Bild.\n",
    "    \n",
    "    Args:\n",
    "        image: Eingabebild\n",
    "        target_size: Zielgröße für die HOG-Feature-Extraktion\n",
    "        \n",
    "    Returns:\n",
    "        hog_image: Visualisierung der HOG-Features\n",
    "    \"\"\"\n",
    "    # Konvertieren in Graustufen\n",
    "    if len(image.shape) == 3:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    else:\n",
    "        gray = image\n",
    "    \n",
    "    # Skalieren auf die Zielgröße\n",
    "    resized = cv2.resize(gray, target_size)\n",
    "    \n",
    "    # HOG-Features extrahieren\n",
    "    features, hog_image = hog(resized, orientations=9, pixels_per_cell=(8, 8),\n",
    "                             cells_per_block=(2, 2), visualize=True, feature_vector=True)\n",
    "    \n",
    "    # Normalisieren für die Visualisierung\n",
    "    hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
    "    \n",
    "    return hog_image_rescaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verbesserte Objekterkennung mit Region Proposals\n",
    "\n",
    "Diese Funktion kombiniert Selective Search für Region Proposals mit dem CNN-Modell zur Klassifikation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_cars_with_region_proposals(image, model, confidence_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Erkennt Autos in einem Bild mit Selective Search für Region Proposals.\n",
    "    \n",
    "    Args:\n",
    "        image: Eingabebild\n",
    "        model: Trainiertes Modell\n",
    "        confidence_threshold: Schwellenwert für die Konfidenz\n",
    "        \n",
    "    Returns:\n",
    "        detections: Liste der erkannten Autos (x, y, w, h, confidence)\n",
    "    \"\"\"\n",
    "    # Regionen mit Selective Search vorschlagen\n",
    "    regions = selective_search_regions(image)\n",
    "    \n",
    "    detections = []\n",
    "    \n",
    "    for x, y, w, h in regions:\n",
    "        # Extrahieren der Region\n",
    "        region = image[y:y+h, x:x+w]\n",
    "        \n",
    "        # Vorverarbeitung der Region\n",
    "        try:\n",
    "            region_resized = cv2.resize(region, (32, 32))\n",
    "            region_normalized = region_resized.astype('float32') / 255.0\n",
    "            region_batch = np.expand_dims(region_normalized, axis=0)\n",
    "            \n",
    "            # Vorhersage\n",
    "            prediction = model.predict(region_batch, verbose=0)[0][0]\n",
    "            \n",
    "            # Wenn die Konfidenz über dem Schwellenwert liegt, speichern wir die Erkennung\n",
    "            if prediction > confidence_threshold:\n",
    "                detections.append((x, y, w, h, prediction))\n",
    "        except Exception as e:\n",
    "            # Ignorieren von Regionen, die nicht verarbeitet werden können\n",
    "            continue\n",
    "    \n",
    "    return detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verbesserte Multi-Scale-Erkennung\n",
    "\n",
    "Diese Funktion kombiniert Region Proposals mit Multi-Scale-Erkennung für eine verbesserte Automerkennung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_cars_multi_scale_improved(image, model, scales=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0, 2.5], \n",
    "                                    confidence_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Erkennt Autos in einem Bild mit verbesserter Multi-Scale-Erkennung.\n",
    "    \n",
    "    Args:\n",
    "        image: Eingabebild\n",
    "        model: Trainiertes Modell\n",
    "        scales: Liste der Skalierungsfaktoren\n",
    "        confidence_threshold: Schwellenwert für die Konfidenz\n",
    "        \n",
    "    Returns:\n",
    "        detections: Liste der erkannten Autos (x, y, w, h, confidence)\n",
    "    \"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    detections = []\n",
    "    \n",
    "    # Erkennung mit Region Proposals auf dem Originalbild\n",
    "    detections.extend(detect_cars_with_region_proposals(image, model, confidence_threshold))\n",
    "    \n",
    "    # Multi-Scale-Erkennung\n",
    "    for scale in scales:\n",
    "        # Skalieren des Bildes\n",
    "        scaled_height = int(height * scale)\n",
    "        scaled_width = int(width * scale)\n",
    "        scaled_image = cv2.resize(image, (scaled_width, scaled_height))\n",
    "        \n",
    "        # Erkennen von Autos im skalierten Bild mit Region Proposals\n",
    "        scaled_detections = detect_cars_with_region_proposals(scaled_image, model, confidence_threshold)\n",
    "        \n",
    "        # Anpassen der Koordinaten an die Originalgröße\n",
    "        for (x, y, w, h, conf) in scaled_detections:\n",
    "            x_orig = int(x / scale)\n",
    "            y_orig = int(y / scale)\n",
    "            w_orig = int(w / scale)\n",
    "            h_orig = int(h / scale)\n",
    "            detections.append((x_orig, y_orig, w_orig, h_orig, conf))\n",
    "    \n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_large_cars(image, model, confidence_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Erkennt große Autos, die einen signifikanten Teil des Bildes einnehmen.\n",
    "    \n",
    "    Args:\n",
    "        image: Eingabebild\n",
    "        model: Trainiertes Modell\n",
    "        confidence_threshold: Schwellenwert für die Konfidenz\n",
    "        \n",
    "    Returns:\n",
    "        detections: Liste der erkannten großen Autos (x, y, w, h, confidence)\n",
    "    \"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    detections = []\n",
    "    \n",
    "    # Definieren von großen Regionen, die signifikante Teile des Bildes abdecken\n",
    "    # Wir erstellen Regionen, die 40% bis 90% des Bildes abdecken\n",
    "    coverage_ratios = [0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    \n",
    "    for ratio in coverage_ratios:\n",
    "        # Berechnen der Größe der Region basierend auf dem Verhältnis\n",
    "        region_width = int(width * ratio)\n",
    "        region_height = int(height * ratio)\n",
    "        \n",
    "        # Berechnen der Position der Region (zentriert)\n",
    "        x = (width - region_width) // 2\n",
    "        y = (height - region_height) // 2\n",
    "        \n",
    "        # Extrahieren der Region\n",
    "        region = image[y:y+region_height, x:x+region_width]\n",
    "        \n",
    "        # Vorverarbeitung der Region\n",
    "        try:\n",
    "            region_resized = cv2.resize(region, (32, 32))\n",
    "            region_normalized = region_resized.astype('float32') / 255.0\n",
    "            region_batch = np.expand_dims(region_normalized, axis=0)\n",
    "            \n",
    "            # Vorhersage\n",
    "            prediction = model.predict(region_batch, verbose=0)[0][0]\n",
    "            \n",
    "            # Wenn die Konfidenz über dem Schwellenwert liegt, speichern wir die Erkennung\n",
    "            if prediction > confidence_threshold:\n",
    "                detections.append((x, y, region_width, region_height, prediction))\n",
    "        except Exception as e:\n",
    "            # Ignorieren von Regionen, die nicht verarbeitet werden können\n",
    "            continue\n",
    "    \n",
    "    return detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verbesserte Non-Maximum Suppression\n",
    "\n",
    "Diese Funktion verwendet einen verbesserten Algorithmus für Non-Maximum Suppression, um überlappende Bounding Boxes zu entfernen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improved_non_max_suppression(boxes, overlap_threshold=0.6, score_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Führt eine verbesserte Non-Maximum Suppression durch, um überlappende Bounding Boxes zu entfernen.\n",
    "    \n",
    "    Args:\n",
    "        boxes: Liste der Bounding Boxes (x, y, w, h, confidence)\n",
    "        overlap_threshold: Schwellenwert für die Überlappung\n",
    "        score_threshold: Minimaler Konfidenzwert\n",
    "        \n",
    "    Returns:\n",
    "        picked: Liste der ausgewählten Bounding Boxes\n",
    "    \"\"\"\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Filtern nach Konfidenz\n",
    "    boxes = [box for box in boxes if box[4] >= score_threshold]\n",
    "    \n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Konvertieren der Bounding Boxes in das Format (x1, y1, x2, y2, conf)\n",
    "    boxes_array = np.array([(x, y, x + w, y + h, conf) for x, y, w, h, conf in boxes])\n",
    "    \n",
    "    # Sortieren der Bounding Boxes nach Konfidenz (absteigend)\n",
    "    boxes_array = boxes_array[np.argsort(boxes_array[:, 4])[::-1]]\n",
    "    \n",
    "    picked = []\n",
    "    \n",
    "    while len(boxes_array) > 0:\n",
    "        # Die Box mit der höchsten Konfidenz auswählen\n",
    "        current_box = boxes_array[0]\n",
    "        picked.append(current_box)\n",
    "        \n",
    "        # Berechnen der Überlappung mit den verbleibenden Boxen\n",
    "        remaining_boxes = boxes_array[1:]\n",
    "        \n",
    "        if len(remaining_boxes) == 0:\n",
    "            break\n",
    "        \n",
    "        # Berechnen der Koordinaten der Überlappung\n",
    "        xx1 = np.maximum(current_box[0], remaining_boxes[:, 0])\n",
    "        yy1 = np.maximum(current_box[1], remaining_boxes[:, 1])\n",
    "        xx2 = np.minimum(current_box[2], remaining_boxes[:, 2])\n",
    "        yy2 = np.minimum(current_box[3], remaining_boxes[:, 3])\n",
    "        \n",
    "        # Berechnen der Breite und Höhe der Überlappung\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    "        \n",
    "        # Berechnen des Überlappungsverhältnisses (IoU - Intersection over Union)\n",
    "        intersection = w * h\n",
    "        area1 = (current_box[2] - current_box[0] + 1) * (current_box[3] - current_box[1] + 1)\n",
    "        area2 = (remaining_boxes[:, 2] - remaining_boxes[:, 0] + 1) * (remaining_boxes[:, 3] - remaining_boxes[:, 1] + 1)\n",
    "        union = area1 + area2 - intersection\n",
    "        iou = intersection / union\n",
    "        \n",
    "        # Entfernen der Boxen mit einer Überlappung über dem Schwellenwert\n",
    "        boxes_array = remaining_boxes[iou < overlap_threshold]\n",
    "    \n",
    "    # Konvertieren zurück in das Format (x, y, w, h, confidence)\n",
    "    picked = [(box[0], box[1], box[2] - box[0], box[3] - box[1], box[4]) for box in picked]\n",
    "    \n",
    "    return picked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_non_max_suppression(boxes, overlap_threshold=0.3, score_threshold=0.6, containment_threshold=0.95):\n",
    "    \"\"\"\n",
    "    Führt eine erweiterte Non-Maximum Suppression durch, die 100% überlappende Boxen entfernt\n",
    "    und eine intelligente Filterung für verschachtelte Boxen implementiert.\n",
    "    \n",
    "    Args:\n",
    "        boxes: Liste der Bounding Boxes (x, y, w, h, confidence)\n",
    "        overlap_threshold: Schwellenwert für die Überlappung (IoU)\n",
    "        score_threshold: Minimaler Konfidenzwert\n",
    "        containment_threshold: Schwellenwert für die Enthaltung einer Box in einer anderen\n",
    "        \n",
    "    Returns:\n",
    "        picked: Liste der ausgewählten Bounding Boxes\n",
    "    \"\"\"\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Filtern nach Konfidenz\n",
    "    boxes = [box for box in boxes if box[4] >= score_threshold]\n",
    "    \n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Konvertieren der Bounding Boxes in das Format (x1, y1, x2, y2, conf)\n",
    "    boxes_array = np.array([(x, y, x + w, y + h, conf) for x, y, w, h, conf in boxes])\n",
    "    \n",
    "    # Sortieren der Bounding Boxes nach Konfidenz (absteigend)\n",
    "    boxes_array = boxes_array[np.argsort(boxes_array[:, 4])[::-1]]\n",
    "    \n",
    "    # Entfernen von 100% überlappenden Boxen (Duplikate)\n",
    "    unique_boxes = []\n",
    "    for box in boxes_array:\n",
    "        is_duplicate = False\n",
    "        for unique_box in unique_boxes:\n",
    "            if (box[0] == unique_box[0] and box[1] == unique_box[1] and \n",
    "                box[2] == unique_box[2] and box[3] == unique_box[3]):\n",
    "                is_duplicate = True\n",
    "                break\n",
    "        if not is_duplicate:\n",
    "            unique_boxes.append(box)\n",
    "    \n",
    "    boxes_array = np.array(unique_boxes)\n",
    "    \n",
    "    picked = []\n",
    "    \n",
    "    while len(boxes_array) > 0:\n",
    "        # Die Box mit der höchsten Konfidenz auswählen\n",
    "        current_box = boxes_array[0]\n",
    "        picked.append(current_box)\n",
    "        \n",
    "        # Berechnen der Überlappung mit den verbleibenden Boxen\n",
    "        remaining_boxes = boxes_array[1:]\n",
    "        \n",
    "        if len(remaining_boxes) == 0:\n",
    "            break\n",
    "        \n",
    "        # Berechnen der Koordinaten der Überlappung\n",
    "        xx1 = np.maximum(current_box[0], remaining_boxes[:, 0])\n",
    "        yy1 = np.maximum(current_box[1], remaining_boxes[:, 1])\n",
    "        xx2 = np.minimum(current_box[2], remaining_boxes[:, 2])\n",
    "        yy2 = np.minimum(current_box[3], remaining_boxes[:, 3])\n",
    "        \n",
    "        # Berechnen der Breite und Höhe der Überlappung\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    "        \n",
    "        # Berechnen des Überlappungsverhältnisses (IoU - Intersection over Union)\n",
    "        intersection = w * h\n",
    "        area1 = (current_box[2] - current_box[0] + 1) * (current_box[3] - current_box[1] + 1)\n",
    "        area2 = (remaining_boxes[:, 2] - remaining_boxes[:, 0] + 1) * (remaining_boxes[:, 3] - remaining_boxes[:, 1] + 1)\n",
    "        union = area1 + area2 - intersection\n",
    "        iou = intersection / union\n",
    "        \n",
    "        # Berechnen des Verhältnisses der Überlappung zur Fläche der kleineren Box\n",
    "        # Dies hilft zu erkennen, ob eine Box fast vollständig in einer anderen enthalten ist\n",
    "        containment_ratio1 = intersection / area1  # Wie viel von Box 1 ist in der Überlappung enthalten\n",
    "        containment_ratio2 = intersection / area2  # Wie viel von Box 2 ist in der Überlappung enthalten\n",
    "        \n",
    "        # Indizes der Boxen, die entfernt werden sollen\n",
    "        to_remove = []\n",
    "        \n",
    "        for i in range(len(remaining_boxes)):\n",
    "            # Wenn die IoU über dem Schwellenwert liegt, entfernen wir die Box\n",
    "            if iou[i] > overlap_threshold:\n",
    "                to_remove.append(i)\n",
    "            # Wenn eine Box fast vollständig in der aktuellen Box enthalten ist\n",
    "            elif containment_ratio2[i] > containment_threshold:\n",
    "                # Wenn die kleinere Box eine höhere Konfidenz hat, behalten wir sie\n",
    "                if remaining_boxes[i, 4] > current_box[4] * 1.2:  # 20% höhere Konfidenz\n",
    "                    continue\n",
    "                to_remove.append(i)\n",
    "        \n",
    "        # Erstellen einer Maske für die zu behaltenden Boxen\n",
    "        mask = np.ones(len(remaining_boxes), dtype=bool)\n",
    "        mask[to_remove] = False\n",
    "        \n",
    "        # Aktualisieren der verbleibenden Boxen\n",
    "        boxes_array = remaining_boxes[mask]\n",
    "    \n",
    "    # Konvertieren zurück in das Format (x, y, w, h, confidence)\n",
    "    picked = [(box[0], box[1], box[2] - box[0], box[3] - box[1], box[4]) for box in picked]\n",
    "    \n",
    "    return picked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zeichnen der Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(image, boxes):\n",
    "    \"\"\"\n",
    "    Zeichnet Bounding Boxes auf ein Bild.\n",
    "    \n",
    "    Args:\n",
    "        image: Eingabebild\n",
    "        boxes: Liste der Bounding Boxes (x, y, w, h, confidence)\n",
    "        \n",
    "    Returns:\n",
    "        result: Bild mit Bounding Boxes\n",
    "    \"\"\"\n",
    "    result = image.copy()\n",
    "    \n",
    "    for (x, y, w, h, conf) in boxes:\n",
    "        # Convert coordinates to integers to avoid TypeError\n",
    "        x, y, w, h = int(x), int(y), int(w), int(h)\n",
    "        \n",
    "        # Zeichnen der Bounding Box\n",
    "        cv2.rectangle(result, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        \n",
    "        # Zeichnen des Konfidenzwerts\n",
    "        label = f\"Car: {conf:.2f}\"\n",
    "        cv2.putText(result, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hauptfunktion zur verbesserten Erkennung von Autos in Bildern\n",
    "\n",
    "Diese Funktion kombiniert alle vorherigen Funktionen, um Autos in einem Bild zu erkennen und zu markieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_draw_cars_improved(image_path, model, output_path):\n",
    "    \"\"\"\n",
    "    Erkennt Autos in einem Bild mit verbesserter Methode und zeichnet Bounding Boxes.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Pfad zum Bild oder URL\n",
    "        model: Trainiertes Modell\n",
    "        output_path: Pfad zum Ausgabebild\n",
    "        \n",
    "    Returns:\n",
    "        boxes: Liste der erkannten Autos (x, y, w, h, confidence)\n",
    "    \"\"\"\n",
    "    # Laden und Vorverarbeiten des Bildes\n",
    "    image, processed_image, (original_height, original_width) = load_and_preprocess_image(image_path)\n",
    "    \n",
    "    # Zeitmessung starten\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Erkennen von Autos im Bild mit verbesserter Multi-Scale-Erkennung\n",
    "    boxes = detect_cars_multi_scale_improved(image, model, confidence_threshold=0.6)\n",
    "    \n",
    "    # Erkennen von großen Autos, die einen signifikanten Teil des Bildes einnehmen\n",
    "    large_car_boxes = detect_large_cars(image, model, confidence_threshold=0.6)\n",
    "    boxes.extend(large_car_boxes)\n",
    "    \n",
    "    # Zusammenführen überlappender Bounding Boxes mit erweiterter NMS\n",
    "    boxes = advanced_non_max_suppression(boxes, overlap_threshold=0.3, score_threshold=0.6, containment_threshold=0.95)\n",
    "    \n",
    "    # Zeitmessung beenden\n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    \n",
    "    # Zeichnen der Bounding Boxes\n",
    "    result = draw_boxes(image, boxes)\n",
    "    \n",
    "    # Hinzufügen von Informationen zum Bild\n",
    "    info_text = f\"Erkannte Autos: {len(boxes)} | Verarbeitungszeit: {processing_time:.2f}s\"\n",
    "    cv2.putText(result, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "    \n",
    "    # Speichern des Ergebnisses\n",
    "    result_rgb = cv2.cvtColor(result, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(output_path, result_rgb)\n",
    "    \n",
    "    # Erstellen einzelner Bilder für jedes erkannte Auto\n",
    "    for i, (x, y, w, h, conf) in enumerate(boxes):\n",
    "        # Convert coordinates to integers to avoid TypeError\n",
    "        x, y, w, h = int(x), int(y), int(w), int(h)\n",
    "        car_image = image[y:y+h, x:x+w]\n",
    "        car_image_with_box = car_image.copy()\n",
    "        cv2.rectangle(car_image_with_box, (0, 0), (w, h), (0, 255, 0), 2)\n",
    "        \n",
    "        # Speichern des Bildes\n",
    "        car_output_path = output_path.replace('.jpg', f'_car_{i+1}.jpg')\n",
    "        car_image_rgb = cv2.cvtColor(car_image_with_box, cv2.COLOR_RGB2BGR)\n",
    "        cv2.imwrite(car_output_path, car_image_rgb)\n",
    "    \n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anwendung auf Testbilder\n",
    "\n",
    "### Testen der verbesserten Automerkennung auf den Bildern aus dem Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testen der verbesserten Automerkennung auf den Bildern aus dem Repository...\n",
      "cv2.ximgproc nicht verfügbar, verwende alternative Methode...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-21 19:09:11.462845: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at cast_op.cc:119 : NOT_FOUND: Cast string to float is not supported\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, image_path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(repo_images):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Erkennen von Autos im Bild\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(results_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbild\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_result_improved.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m     boxes \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_and_draw_cars_improved\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBild \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(boxes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Autos erkannt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Anzeigen des Ergebnisses im Notebook\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[31], line 20\u001b[0m, in \u001b[0;36mdetect_and_draw_cars_improved\u001b[0;34m(image_path, model, output_path)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Erkennen von großen Autos, die einen signifikanten Teil des Bildes einnehmen\u001b[39;00m\n\u001b[1;32m     18\u001b[0m large_car_boxes \u001b[38;5;241m=\u001b[39m detect_large_cars(image, model, confidence_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m)\n\u001b[1;32m     19\u001b[0m boxes\u001b[38;5;241m.\u001b[39mextend(large_car_boxes)\n\u001b[0;32m---> 20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Zusammenführen überlappender Bounding Boxes mit erweiterter NMS\u001b[39;00m\n\u001b[1;32m     22\u001b[0m boxes \u001b[38;5;241m=\u001b[39m \u001b[43madvanced_non_max_suppression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverlap_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontainment_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Zeitmessung beenden\u001b[39;00m\n\u001b[1;32m     25\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print(\"Testen der verbesserten Automerkennung auf den Bildern aus dem Repository...\")\n",
    "\n",
    "# Bilder aus dem Repository\n",
    "repo_images = [\n",
    "    os.path.join(images_dir, 'bild1.jpg'),\n",
    "    os.path.join(images_dir, 'bild2.jpg'),\n",
    "    os.path.join(images_dir, 'bild3.jpg')\n",
    "]\n",
    "\n",
    "for i, image_path in enumerate(repo_images):\n",
    "    # Erkennen von Autos im Bild\n",
    "    output_path = os.path.join(results_dir, f'bild{i+1}_result_improved.jpg')\n",
    "    boxes = detect_and_draw_cars_improved(image_path, model, output_path)\n",
    "    \n",
    "    print(f\"Bild {i+1}: {len(boxes)} Autos erkannt\")\n",
    "    \n",
    "    # Anzeigen des Ergebnisses im Notebook\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(plt.imread(output_path))\n",
    "    plt.title(f\"Bild {i+1}: {len(boxes)} Autos erkannt\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testen der verbesserten Automerkennung auf zusätzlichen Bildern aus dem Internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testen der verbesserten Automerkennung auf zusätzlichen Bildern aus dem Internet...\")\n",
    "\n",
    "# Suchen nach Bildern mit mehreren Autos\n",
    "additional_images = [\n",
    "    \"https://cdn.pixabay.com/photo/2017/11/23/04/13/traffic-jam-2972156_1280.jpg\",\n",
    "    \"https://cdn.pixabay.com/photo/2017/08/01/09/34/car-2563902_1280.jpg\",\n",
    "    \"https://cdn.pixabay.com/photo/2017/08/06/12/08/car-2592150_1280.jpg\"\n",
    "]\n",
    "\n",
    "for i, image_url in enumerate(additional_images):\n",
    "    # Speichern des Bildes\n",
    "    image_path = os.path.join(images_dir, f'additional_image_{i+1}.jpg')\n",
    "    \n",
    "    # Herunterladen des Bildes, wenn es noch nicht existiert\n",
    "    if not os.path.exists(image_path):\n",
    "        response = requests.get(image_url)\n",
    "        with open(image_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "    \n",
    "    # Erkennen von Autos im Bild\n",
    "    output_path = os.path.join(results_dir, f'additional_image_{i+1}_result_improved.jpg')\n",
    "    boxes = detect_and_draw_cars_improved(image_path, model, output_path)\n",
    "    \n",
    "    print(f\"Zusätzliches Bild {i+1}: {len(boxes)} Autos erkannt\")\n",
    "    \n",
    "    # Anzeigen des Ergebnisses im Notebook\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(plt.imread(output_path))\n",
    "    plt.title(f\"Zusätzliches Bild {i+1}: {len(boxes)} Autos erkannt\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vergleich mit der ursprünglichen Methode\n",
    "\n",
    "Hier vergleichen wir die Ergebnisse der verbesserten Methode mit der ursprünglichen Sliding-Window-Methode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_cars(image, model, window_size=(64, 64), stride=32, confidence_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Erkennt Autos in einem Bild mit Sliding Window.\n",
    "    \n",
    "    Args:\n",
    "        image: Eingabebild\n",
    "        model: Trainiertes Modell\n",
    "        window_size: Größe des Sliding Windows\n",
    "        stride: Schrittweite des Sliding Windows\n",
    "        confidence_threshold: Schwellenwert für die Konfidenz\n",
    "        \n",
    "    Returns:\n",
    "        detections: Liste der erkannten Autos (x, y, w, h, confidence)\n",
    "    \"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    detections = []\n",
    "    \n",
    "    for y in range(0, height - window_size[1], stride):\n",
    "        for x in range(0, width - window_size[0], stride):\n",
    "            # Extrahieren des Fensters\n",
    "            window = image[y:y + window_size[1], x:x + window_size[0]]\n",
    "            \n",
    "            # Vorverarbeitung des Fensters\n",
    "            window = cv2.resize(window, (32, 32))\n",
    "            window = window.astype('float32') / 255.0\n",
    "            window = np.expand_dims(window, axis=0)\n",
    "            \n",
    "            # Vorhersage\n",
    "            prediction = model.predict(window, verbose=0)[0][0]\n",
    "            \n",
    "            # Wenn die Konfidenz über dem Schwellenwert liegt, speichern wir die Erkennung\n",
    "            if prediction > confidence_threshold:\n",
    "                detections.append((x, y, window_size[0], window_size[1], prediction))\n",
    "    \n",
    "    return detections\n",
    "\n",
    "def detect_cars_multi_scale(image, model, scales=[0.5, 0.75, 1.0, 1.25, 1.5, 2.0, 2.5], \n",
    "                           window_size=(64, 64), stride=32, confidence_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Erkennt Autos in einem Bild mit Multi-Scale Sliding Window.\n",
    "    \n",
    "    Args:\n",
    "        image: Eingabebild\n",
    "        model: Trainiertes Modell\n",
    "        scales: Liste der Skalierungsfaktoren\n",
    "        window_size: Größe des Sliding Windows\n",
    "        stride: Schrittweite des Sliding Windows\n",
    "        confidence_threshold: Schwellenwert für die Konfidenz\n",
    "        \n",
    "    Returns:\n",
    "        detections: Liste der erkannten Autos (x, y, w, h, confidence)\n",
    "    \"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    detections = []\n",
    "    \n",
    "    for scale in scales:\n",
    "        # Skalieren des Bildes\n",
    "        scaled_height = int(height * scale)\n",
    "        scaled_width = int(width * scale)\n",
    "        scaled_image = cv2.resize(image, (scaled_width, scaled_height))\n",
    "        \n",
    "        # Erkennen von Autos im skalierten Bild\n",
    "        scaled_detections = detect_cars(scaled_image, model, window_size, stride, confidence_threshold)\n",
    "        \n",
    "        # Anpassen der Koordinaten an die Originalgröße\n",
    "        for (x, y, w, h, conf) in scaled_detections:\n",
    "            x_orig = int(x / scale)\n",
    "            y_orig = int(y / scale)\n",
    "            w_orig = int(w / scale)\n",
    "            h_orig = int(h / scale)\n",
    "            detections.append((x_orig, y_orig, w_orig, h_orig, conf))\n",
    "    \n",
    "    return detections\n",
    "\n",
    "def non_max_suppression(boxes, overlap_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Führt Non-Maximum Suppression durch, um überlappende Bounding Boxes zu entfernen.\n",
    "    \n",
    "    Args:\n",
    "        boxes: Liste der Bounding Boxes (x, y, w, h, confidence)\n",
    "        overlap_threshold: Schwellenwert für die Überlappung\n",
    "        \n",
    "    Returns:\n",
    "        picked: Liste der ausgewählten Bounding Boxes\n",
    "    \"\"\"\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Konvertieren der Bounding Boxes in das Format (x1, y1, x2, y2)\n",
    "    boxes_array = np.array([(x, y, x + w, y + h, conf) for x, y, w, h, conf in boxes])\n",
    "    \n",
    "    # Sortieren der Bounding Boxes nach Konfidenz (absteigend)\n",
    "    boxes_array = boxes_array[np.argsort(boxes_array[:, 4])[::-1]]\n",
    "    \n",
    "    picked = []\n",
    "    \n",
    "    while len(boxes_array) > 0:\n",
    "        # Die Box mit der höchsten Konfidenz auswählen\n",
    "        current_box = boxes_array[0]\n",
    "        picked.append(current_box)\n",
    "        \n",
    "        # Berechnen der Überlappung mit den verbleibenden Boxen\n",
    "        remaining_boxes = boxes_array[1:]\n",
    "        \n",
    "        if len(remaining_boxes) == 0:\n",
    "            break\n",
    "        \n",
    "        # Berechnen der Koordinaten der Überlappung\n",
    "        xx1 = np.maximum(current_box[0], remaining_boxes[:, 0])\n",
    "        yy1 = np.maximum(current_box[1], remaining_boxes[:, 1])\n",
    "        xx2 = np.minimum(current_box[2], remaining_boxes[:, 2])\n",
    "        yy2 = np.minimum(current_box[3], remaining_boxes[:, 3])\n",
    "        \n",
    "        # Berechnen der Breite und Höhe der Überlappung\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    "        \n",
    "        # Berechnen des Überlappungsverhältnisses\n",
    "        overlap = (w * h) / ((remaining_boxes[:, 2] - remaining_boxes[:, 0] + 1) * \n",
    "                             (remaining_boxes[:, 3] - remaining_boxes[:, 1] + 1))\n",
    "        \n",
    "        # Entfernen der Boxen mit einer Überlappung über dem Schwellenwert\n",
    "        boxes_array = remaining_boxes[overlap < overlap_threshold]\n",
    "    \n",
    "    # Konvertieren zurück in das Format (x, y, w, h, confidence)\n",
    "    picked = [(box[0], box[1], box[2] - box[0], box[3] - box[1], box[4]) for box in picked]\n",
    "    \n",
    "    return picked\n",
    "\n",
    "def detect_and_draw_cars_original(image_path, model, output_path, multi_scale=True):\n",
    "    \"\"\"\n",
    "    Erkennt Autos in einem Bild und zeichnet Bounding Boxes mit der ursprünglichen Methode.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Pfad zum Bild oder URL\n",
    "        model: Trainiertes Modell\n",
    "        output_path: Pfad zum Ausgabebild\n",
    "        multi_scale: Ob Multi-Scale Sliding Window verwendet werden soll\n",
    "        \n",
    "    Returns:\n",
    "        boxes: Liste der erkannten Autos (x, y, w, h, confidence)\n",
    "    \"\"\"\n",
    "    # Laden und Vorverarbeiten des Bildes\n",
    "    image, processed_image, (original_height, original_width) = load_and_preprocess_image(image_path)\n",
    "    \n",
    "    # Zeitmessung starten\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Erkennen von Autos im Bild\n",
    "    if multi_scale:\n",
    "        boxes = detect_cars_multi_scale(image, model)\n",
    "    else:\n",
    "        boxes = detect_cars(image, model)\n",
    "    \n",
    "    # Zusammenführen überlappender Bounding Boxes\n",
    "    boxes = non_max_suppression(boxes)\n",
    "    \n",
    "    # Zeitmessung beenden\n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    \n",
    "    # Zeichnen der Bounding Boxes\n",
    "    result = draw_boxes(image, boxes)\n",
    "    \n",
    "    # Hinzufügen von Informationen zum Bild\n",
    "    info_text = f\"Erkannte Autos: {len(boxes)} | Verarbeitungszeit: {processing_time:.2f}s\"\n",
    "    cv2.putText(result, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "    \n",
    "    # Speichern des Ergebnisses\n",
    "    result_rgb = cv2.cvtColor(result, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(output_path, result_rgb)\n",
    "    \n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vergleich der verbesserten Methode mit der ursprünglichen Methode...\")\n",
    "\n",
    "# Bilder aus dem Repository für den Vergleich\n",
    "for i, image_path in enumerate(repo_images):\n",
    "    # Erkennen von Autos im Bild mit der ursprünglichen Methode\n",
    "    output_path_original = os.path.join(results_dir, f'bild{i+1}_result_original.jpg')\n",
    "    boxes_original = detect_and_draw_cars_original(image_path, model, output_path_original)\n",
    "    \n",
    "    # Erkennen von Autos im Bild mit der verbesserten Methode\n",
    "    output_path_improved = os.path.join(results_dir, f'bild{i+1}_result_improved.jpg')\n",
    "    \n",
    "    # Anzeigen des Vergleichs im Notebook\n",
    "    plt.figure(figsize=(18, 8))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(plt.imread(output_path_original))\n",
    "    plt.title(f\"Original: {len(boxes_original)} Autos erkannt\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(plt.imread(output_path_improved))\n",
    "    plt.title(f\"Verbessert: {len(boxes)} Autos erkannt\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zusammenfassung\n",
    "\n",
    "In diesem Notebook haben wir:\n",
    "1. Ein trainiertes CNN-Modell zur Autoerkennung geladen\n",
    "2. Einen verbesserten Ansatz zur Automerkennung implementiert, der Selective Search für Region Proposals verwendet\n",
    "3. Eine verbesserte Multi-Scale-Erkennung implementiert, um Autos unterschiedlicher Größen zuverlässiger zu erkennen\n",
    "4. Eine spezielle Erkennung für sehr große Autos implementiert, die einen signifikanten Teil des Bildes einnehmen\n",
    "5. Eine erweiterte Non-Maximum Suppression implementiert, die 100% überlappende Boxen entfernt und eine intelligente Filterung für verschachtelte Boxen bietet\n",
    "6. Die Erkennung auf verschiedenen Testbildern angewendet und die Ergebnisse visualisiert\n",
    "7. Die Ergebnisse der verbesserten Methode mit der ursprünglichen Sliding-Window-Methode verglichen\n",
    "\n",
    "Der implementierte verbesserte Ansatz ermöglicht eine zuverlässigere Erkennung von Autos in Bildern unterschiedlicher Größen und Perspektiven. Die Verwendung von Selective Search für Region Proposals anstelle des Sliding-Window-Ansatzes führt zu einer besseren Erkennung von Autos, insbesondere in komplexen Szenen mit mehreren Autos. Die spezielle Erkennung für sehr große Autos ermöglicht es, Autos zu erkennen, die einen signifikanten Teil des Bildes einnehmen. Die erweiterte Non-Maximum Suppression verhindert 100% überlappende Boxen und bietet eine intelligente Filterung für verschachtelte Boxen, die nicht einfach kleinere Boxen innerhalb größerer Boxen entfernt, sondern die Konfidenz und das Enthaltensein berücksichtigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Verbesserte Automerkennung auf Bildern abgeschlossen. Die Ergebnisse wurden im Verzeichnis 'results' gespeichert.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
