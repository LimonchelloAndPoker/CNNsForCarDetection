{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benutzerdefiniertes CNN ohne keras.models oder keras.layers\n",
    "\n",
    "Dieses Notebook implementiert ein Convolutional Neural Network (CNN) ohne Verwendung von keras.models oder keras.layers zur Erkennung von Autos im CIFAR-10 Datensatz.\n",
    "\n",
    "## Überblick\n",
    "- Implementierung eines CNN von Grund auf mit NumPy\n",
    "- Implementierung der Vorwärts- und Rückwärtspropagierung für alle Schichten\n",
    "- Training des Modells mit Mini-Batch Gradient Descent\n",
    "- Evaluierung des Modells auf Testdaten\n",
    "- Visualisierung der Ergebnisse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importieren der benötigten Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorbereitung der Verzeichnisse und Laden der Daten\n",
    "\n",
    "Wir laden die vorbereiteten Daten aus dem ersten Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verzeichnisse\n",
    "data_dir = '../data'\n",
    "models_dir = '../models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Laden der vorbereiteten Daten\n",
    "print(\"Laden der vorbereiteten Daten...\")\n",
    "x_train = np.load(os.path.join(data_dir, 'x_train.npy'))\n",
    "y_train = np.load(os.path.join(data_dir, 'y_train.npy'))\n",
    "x_test = np.load(os.path.join(data_dir, 'x_test.npy'))\n",
    "y_test = np.load(os.path.join(data_dir, 'y_test.npy'))\n",
    "y_train_binary = np.load(os.path.join(data_dir, 'y_train_binary.npy')).reshape(-1, 1)\n",
    "y_test_binary = np.load(os.path.join(data_dir, 'y_test_binary.npy')).reshape(-1, 1)\n",
    "\n",
    "print(f\"Trainingsdaten: {x_train.shape[0]} Bilder\")\n",
    "print(f\"Testdaten: {x_test.shape[0]} Bilder\")\n",
    "print(f\"Anzahl der Auto-Bilder im Trainingsdatensatz: {np.sum(y_train_binary)}\")\n",
    "print(f\"Anzahl der Auto-Bilder im Testdatensatz: {np.sum(y_test_binary)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduzieren der Datensatzgröße für schnelleres Training\n",
    "\n",
    "Da die Implementierung eines CNN von Grund auf rechenintensiv ist, reduzieren wir die Datensatzgröße für das Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Für das Training verwenden wir einen kleineren Datensatz, um die Rechenzeit zu reduzieren\n",
    "# Wir verwenden 10% der Trainingsdaten und 10% der Testdaten\n",
    "np.random.seed(42)\n",
    "train_indices = np.random.choice(len(x_train), size=int(len(x_train) * 0.1), replace=False)\n",
    "test_indices = np.random.choice(len(x_test), size=int(len(x_test) * 0.1), replace=False)\n",
    "\n",
    "x_train_small = x_train[train_indices]\n",
    "y_train_small = y_train_binary[train_indices]\n",
    "x_test_small = x_test[test_indices]\n",
    "y_test_small = y_test_binary[test_indices]\n",
    "\n",
    "print(f\"Reduzierte Trainingsdaten: {x_train_small.shape[0]} Bilder\")\n",
    "print(f\"Reduzierte Testdaten: {x_test_small.shape[0]} Bilder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementierung der CNN-Funktionen\n",
    "\n",
    "### Initialisierung der Parameter\n",
    "\n",
    "Wir initialisieren die Parameter für unser CNN-Modell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(filter_sizes, num_filters):\n",
    "    \"\"\"\n",
    "    Initialisiert die Parameter für ein CNN.\n",
    "    \n",
    "    Args:\n",
    "        filter_sizes: Liste der Filter-Größen für jede Schicht\n",
    "        num_filters: Liste der Anzahl der Filter für jede Schicht\n",
    "        \n",
    "    Returns:\n",
    "        parameters: Dictionary mit den initialisierten Parametern\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(num_filters)\n",
    "    \n",
    "    for l in range(1, L + 1):\n",
    "        parameters[f'W{l}'] = np.random.randn(filter_sizes[l-1], filter_sizes[l-1], 3 if l == 1 else num_filters[l-2], num_filters[l-1]) * 0.01\n",
    "        parameters[f'b{l}'] = np.zeros((1, 1, 1, num_filters[l-1]))\n",
    "    \n",
    "    # Fully connected layer\n",
    "    parameters['W_fc'] = np.random.randn(4 * 4 * num_filters[-1], 1) * 0.01\n",
    "    parameters['b_fc'] = np.zeros((1, 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hilfsfunktionen für die Faltungsoperation\n",
    "\n",
    "Wir implementieren Hilfsfunktionen für die Faltungsoperation, wie Zero-Padding und einzelne Faltungsschritte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pad(X, pad):\n",
    "    \"\"\"\n",
    "    Fügt Nullen um die Bilder herum hinzu.\n",
    "    \n",
    "    Args:\n",
    "        X: Eingabedaten der Form (m, h, w, c)\n",
    "        pad: Anzahl der Nullen, die hinzugefügt werden sollen\n",
    "        \n",
    "    Returns:\n",
    "        X_pad: Gepolsterte Eingabedaten\n",
    "    \"\"\"\n",
    "    X_pad = np.pad(X, ((0, 0), (pad, pad), (pad, pad), (0, 0)), mode='constant', constant_values=0)\n",
    "    return X_pad\n",
    "\n",
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    \"\"\"\n",
    "    Führt einen einzelnen Faltungsschritt durch.\n",
    "    \n",
    "    Args:\n",
    "        a_slice_prev: Ausschnitt der Eingabedaten\n",
    "        W: Gewichte\n",
    "        b: Bias\n",
    "        \n",
    "    Returns:\n",
    "        Z: Ergebnis der Faltung\n",
    "    \"\"\"\n",
    "    Z = np.sum(a_slice_prev * W) + float(b)\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vorwärtspropagierung für die Faltungsschicht\n",
    "\n",
    "Wir implementieren die Vorwärtspropagierung für die Faltungsschicht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    \"\"\"\n",
    "    Führt einen Vorwärtsdurchlauf für eine Faltungsschicht durch.\n",
    "    \n",
    "    Args:\n",
    "        A_prev: Ausgabe der vorherigen Schicht (m, h_prev, w_prev, c_prev)\n",
    "        W: Gewichte (f, f, c_prev, c)\n",
    "        b: Bias (1, 1, 1, c)\n",
    "        hparameters: Dictionary mit Hyperparametern\n",
    "        \n",
    "    Returns:\n",
    "        Z: Ausgabe der Faltungsschicht\n",
    "        cache: Cache für die Rückwärtspropagierung\n",
    "    \"\"\"\n",
    "    (m, h_prev, w_prev, c_prev) = A_prev.shape\n",
    "    (f, f, c_prev, c) = W.shape\n",
    "    \n",
    "    stride = hparameters['stride']\n",
    "    pad = hparameters['pad']\n",
    "    \n",
    "    n_H = int((h_prev - f + 2 * pad) / stride) + 1\n",
    "    n_W = int((w_prev - f + 2 * pad) / stride) + 1\n",
    "    \n",
    "    Z = np.zeros((m, n_H, n_W, c))\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    \n",
    "    for i in range(m):\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                for c_out in range(c):\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "                    Z[i, h, w, c_out] = conv_single_step(a_slice_prev, W[:, :, :, c_out], b[:, :, :, c_out])\n",
    "    \n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aktivierungsfunktionen\n",
    "\n",
    "Wir implementieren die ReLU- und Sigmoid-Aktivierungsfunktionen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Wendet die ReLU-Aktivierungsfunktion an.\n",
    "    \n",
    "    Args:\n",
    "        Z: Eingabedaten\n",
    "        \n",
    "    Returns:\n",
    "        A: Ausgabe nach Anwendung von ReLU\n",
    "        cache: Cache für die Rückwärtspropagierung\n",
    "    \"\"\"\n",
    "    A = np.maximum(0, Z)\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Wendet die Sigmoid-Aktivierungsfunktion an.\n",
    "    \n",
    "    Args:\n",
    "        Z: Eingabedaten\n",
    "        \n",
    "    Returns:\n",
    "        A: Ausgabe nach Anwendung von Sigmoid\n",
    "        cache: Cache für die Rückwärtspropagierung\n",
    "    \"\"\"\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling-Schicht\n",
    "\n",
    "Wir implementieren die Vorwärtspropagierung für die Pooling-Schicht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_forward(A_prev, hparameters, mode=\"max\"):\n",
    "    \"\"\"\n",
    "    Führt einen Vorwärtsdurchlauf für eine Pooling-Schicht durch.\n",
    "    \n",
    "    Args:\n",
    "        A_prev: Ausgabe der vorherigen Schicht (m, h_prev, w_prev, c_prev)\n",
    "        hparameters: Dictionary mit Hyperparametern\n",
    "        mode: Pooling-Modus (\"max\" oder \"average\")\n",
    "        \n",
    "    Returns:\n",
    "        A: Ausgabe der Pooling-Schicht\n",
    "        cache: Cache für die Rückwärtspropagierung\n",
    "    \"\"\"\n",
    "    (m, h_prev, w_prev, c_prev) = A_prev.shape\n",
    "    \n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "    \n",
    "    n_H = int(1 + (h_prev - f) / stride)\n",
    "    n_W = int(1 + (w_prev - f) / stride)\n",
    "    n_C = c_prev\n",
    "    \n",
    "    A = np.zeros((m, n_H, n_W, n_C))\n",
    "    \n",
    "    for i in range(m):\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                for c in range(n_C):\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                    \n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = np.mean(a_prev_slice)\n",
    "    \n",
    "    cache = (A_prev, hparameters)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten und Fully Connected Layer\n",
    "\n",
    "Wir implementieren die Funktionen zum Abflachen der Ausgabe der letzten Pooling-Schicht und für die Fully Connected Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(A):\n",
    "    \"\"\"\n",
    "    Flacht die Ausgabe der letzten Pooling-Schicht ab.\n",
    "    \n",
    "    Args:\n",
    "        A: Ausgabe der letzten Pooling-Schicht (m, h, w, c)\n",
    "        \n",
    "    Returns:\n",
    "        A_flat: Abgeflachte Ausgabe (m, h*w*c)\n",
    "    \"\"\"\n",
    "    return A.reshape(A.shape[0], -1)\n",
    "\n",
    "def fc_forward(A_prev, W, b):\n",
    "    \"\"\"\n",
    "    Führt einen Vorwärtsdurchlauf für eine Fully-Connected-Schicht durch.\n",
    "    \n",
    "    Args:\n",
    "        A_prev: Ausgabe der vorherigen Schicht (m, n_prev)\n",
    "        W: Gewichte (n_prev, n)\n",
    "        b: Bias (1, n)\n",
    "        \n",
    "    Returns:\n",
    "        Z: Ausgabe der Fully-Connected-Schicht\n",
    "        cache: Cache für die Rückwärtspropagierung\n",
    "    \"\"\"\n",
    "    Z = np.dot(A_prev, W) + b\n",
    "    cache = (A_prev, W, b)\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kostenfunktion\n",
    "\n",
    "Wir implementieren die binäre Kreuzentropie-Kostenfunktion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Berechnet die Kostenfunktion (binäre Kreuzentropie).\n",
    "    \n",
    "    Args:\n",
    "        AL: Ausgabe des Modells (m, 1)\n",
    "        Y: Tatsächliche Labels (m, 1)\n",
    "        \n",
    "    Returns:\n",
    "        cost: Wert der Kostenfunktion\n",
    "    \"\"\"\n",
    "    m = Y.shape[0]\n",
    "    cost = -1/m * np.sum(Y * np.log(AL + 1e-8) + (1 - Y) * np.log(1 - AL + 1e-8))\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rückwärtspropagierung\n",
    "\n",
    "Wir implementieren die Rückwärtspropagierung für alle Schichten des Modells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Führt einen Rückwärtsdurchlauf für die ReLU-Aktivierungsfunktion durch.\n",
    "    \n",
    "    Args:\n",
    "        dA: Gradient der Kostenfunktion bezüglich der Ausgabe der ReLU-Funktion\n",
    "        cache: Cache aus dem Vorwärtsdurchlauf\n",
    "        \n",
    "    Returns:\n",
    "        dZ: Gradient der Kostenfunktion bezüglich der Eingabe der ReLU-Funktion\n",
    "    \"\"\"\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ\n",
    "\n",
    "def conv_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Führt einen Rückwärtsdurchlauf für eine Faltungsschicht durch.\n",
    "    \n",
    "    Args:\n",
    "        dZ: Gradient der Kostenfunktion bezüglich der Ausgabe der Faltungsschicht\n",
    "        cache: Cache aus dem Vorwärtsdurchlauf\n",
    "        \n",
    "    Returns:\n",
    "        dA_prev: Gradient der Kostenfunktion bezüglich der Eingabe der Faltungsschicht\n",
    "        dW: Gradient der Kostenfunktion bezüglich der Gewichte\n",
    "        db: Gradient der Kostenfunktion bezüglich des Bias\n",
    "    \"\"\"\n",
    "    A_prev, W, b, hparameters = cache\n",
    "    \n",
    "    (m, h_prev, w_prev, c_prev) = A_prev.shape\n",
    "    (f, f, c_prev, c) = W.shape\n",
    "    (m, n_H, n_W, c) = dZ.shape\n",
    "    \n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    \n",
    "    dA_prev = np.zeros((m, h_prev, w_prev, c_prev))\n",
    "    dW = np.zeros((f, f, c_prev, c))\n",
    "    db = np.zeros((1, 1, 1, c))\n",
    "    \n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
    "    \n",
    "    for i in range(m):\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        da_prev_pad = dA_prev_pad[i]\n",
    "        \n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                for c_out in range(c):\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "                    \n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:, :, :, c_out] * dZ[i, h, w, c_out]\n",
    "                    dW[:, :, :, c_out] += a_slice * dZ[i, h, w, c_out]\n",
    "                    db[:, :, :, c_out] += dZ[i, h, w, c_out]\n",
    "        \n",
    "        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def pool_backward(dA, cache, mode=\"max\"):\n",
    "    \"\"\"\n",
    "    Führt einen Rückwärtsdurchlauf für eine Pooling-Schicht durch.\n",
    "    \n",
    "    Args:\n",
    "        dA: Gradient der Kostenfunktion bezüglich der Ausgabe der Pooling-Schicht\n",
    "        cache: Cache aus dem Vorwärtsdurchlauf\n",
    "        mode: Pooling-Modus (\"max\" oder \"average\")\n",
    "        \n",
    "    Returns:\n",
    "        dA_prev: Gradient der Kostenfunktion bezüglich der Eingabe der Pooling-Schicht\n",
    "    \"\"\"\n",
    "    (A_prev, hparameters) = cache\n",
    "    \n",
    "    stride = hparameters[\"stride\"]\n",
    "    f = hparameters[\"f\"]\n",
    "    \n",
    "    m, h_prev, w_prev, c_prev = A_prev.shape\n",
    "    m, h, w, c = dA.shape\n",
    "    \n",
    "    dA_prev = np.zeros_like(A_prev)\n",
    "    \n",
    "    for i in range(m):\n",
    "        a_prev = A_prev[i]\n",
    "        for h in range(h):\n",
    "            for w in range(w):\n",
    "                for c in range(c):\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    if mode == \"max\":\n",
    "                        a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                        if a_prev_slice.size > 0:  # Überprüfen, ob das Array nicht leer ist\n",
    "                            mask = (a_prev_slice == np.max(a_prev_slice))\n",
    "                            dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += mask * dA[i, h, w, c]\n",
    "                    elif mode == \"average\":\n",
    "                        da = dA[i, h, w, c]\n",
    "                        size = (vert_end - vert_start) * (horiz_end - horiz_start)\n",
    "                        if size > 0:  # Überprüfen, ob die Größe nicht null ist\n",
    "                            dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += da / size\n",
    "    \n",
    "    return dA_prev\n",
    "\n",
    "def fc_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Führt einen Rückwärtsdurchlauf für eine Fully-Connected-Schicht durch.\n",
    "    \n",
    "    Args:\n",
    "        dZ: Gradient der Kostenfunktion bezüglich der Ausgabe der Fully-Connected-Schicht\n",
    "        cache: Cache aus dem Vorwärtsdurchlauf\n",
    "        \n",
    "    Returns:\n",
    "        dA_prev: Gradient der Kostenfunktion bezüglich der Eingabe der Fully-Connected-Schicht\n",
    "        dW: Gradient der Kostenfunktion bezüglich der Gewichte\n",
    "        db: Gradient der Kostenfunktion bezüglich des Bias\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[0]\n",
    "    \n",
    "    dW = 1/m * np.dot(A_prev.T, dZ)\n",
    "    db = 1/m * np.sum(dZ, axis=0, keepdims=True)\n",
    "    dA_prev = np.dot(dZ, W.T)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Führt einen Rückwärtsdurchlauf für die Sigmoid-Aktivierungsfunktion durch.\n",
    "    \n",
    "    Args:\n",
    "        dA: Gradient der Kostenfunktion bezüglich der Ausgabe der Sigmoid-Funktion\n",
    "        cache: Cache aus dem Vorwärtsdurchlauf\n",
    "        \n",
    "    Returns:\n",
    "        dZ: Gradient der Kostenfunktion bezüglich der Eingabe der Sigmoid-Funktion\n",
    "    \"\"\"\n",
    "    Z = cache\n",
    "    s = 1 / (1 + np.exp(-Z))\n",
    "    dZ = dA * s * (1 - s)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vorwärts- und Rückwärtspropagierung für das gesamte Modell\n",
    "\n",
    "Wir implementieren die Vorwärts- und Rückwärtspropagierung für das gesamte Modell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Führt einen Vorwärtsdurchlauf für das gesamte Modell durch.\n",
    "    \n",
    "    Args:\n",
    "        X: Eingabedaten (m, h, w, c)\n",
    "        parameters: Dictionary mit den Parametern des Modells\n",
    "        \n",
    "    Returns:\n",
    "        AL: Ausgabe des Modells\n",
    "        caches: Liste der Caches für die Rückwärtspropagierung\n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2 - 1  # Anzahl der Faltungsschichten\n",
    "    \n",
    "    # Faltungsschichten\n",
    "    for l in range(1, L + 1):\n",
    "        A_prev = A\n",
    "        \n",
    "        # Faltung\n",
    "        Z, conv_cache = conv_forward(A_prev, parameters[f'W{l}'], parameters[f'b{l}'], \n",
    "                                    {'stride': 1, 'pad': 1})\n",
    "        \n",
    "        # ReLU\n",
    "        A, relu_cache = relu(Z)\n",
    "        \n",
    "        # Pooling\n",
    "        A, pool_cache = pool_forward(A, {'stride': 2, 'f': 2}, mode=\"max\")\n",
    "        \n",
    "        caches.append((conv_cache, relu_cache, pool_cache))\n",
    "    \n",
    "    # Flatten\n",
    "    A_flat = flatten(A)\n",
    "    \n",
    "    # Fully connected layer\n",
    "    Z_fc, fc_cache = fc_forward(A_flat, parameters['W_fc'], parameters['b_fc'])\n",
    "    \n",
    "    # Sigmoid\n",
    "    AL, sigmoid_cache = sigmoid(Z_fc)\n",
    "    \n",
    "    caches.append((fc_cache, sigmoid_cache))\n",
    "    \n",
    "    return AL, caches\n",
    "\n",
    "def model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Führt einen Rückwärtsdurchlauf für das gesamte Modell durch.\n",
    "    \n",
    "    Args:\n",
    "        AL: Ausgabe des Modells\n",
    "        Y: Tatsächliche Labels\n",
    "        caches: Liste der Caches aus dem Vorwärtsdurchlauf\n",
    "        \n",
    "    Returns:\n",
    "        gradients: Dictionary mit den Gradienten\n",
    "    \"\"\"\n",
    "    gradients = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[0]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    # Initialisierung des Gradienten der Ausgabeschicht\n",
    "    dAL = - (np.divide(Y, AL + 1e-8) - np.divide(1 - Y, 1 - AL + 1e-8))\n",
    "    \n",
    "    # Rückwärtsdurchlauf für die Fully-Connected-Schicht\n",
    "    fc_cache, sigmoid_cache = caches[L-1]\n",
    "    dZ_fc = sigmoid_backward(dAL, sigmoid_cache)\n",
    "    dA_flat, dW_fc, db_fc = fc_backward(dZ_fc, fc_cache)\n",
    "    \n",
    "    gradients['dW_fc'] = dW_fc\n",
    "    gradients['db_fc'] = db_fc\n",
    "    \n",
    "    # Reshape dA_flat zurück in die Form der letzten Pooling-Schicht\n",
    "    # Berechne die korrekte Form basierend auf dem letzten Pool-Cache\n",
    "    last_pool_shape = caches[L-2][2][0].shape\n",
    "    # Prüfe, ob die Dimensionen kompatibel sind\n",
    "    if dA_flat.size != np.prod(last_pool_shape):\n",
    "        # Wenn nicht kompatibel, verwende eine sichere Reshape-Operation\n",
    "        # Berechne die neue Form basierend auf der Größe von dA_flat\n",
    "        n_samples = last_pool_shape[0]\n",
    "        height = last_pool_shape[1]\n",
    "        width = last_pool_shape[2]\n",
    "        n_channels = dA_flat.size // (n_samples * height * width)\n",
    "        dA = dA_flat.reshape(n_samples, height, width, n_channels)\n",
    "    else:\n",
    "        # Wenn kompatibel, verwende die ursprüngliche Form\n",
    "        dA = dA_flat.reshape(last_pool_shape)\n",
    "    \n",
    "    # Rückwärtsdurchlauf für die Faltungsschichten\n",
    "    for l in reversed(range(L-1)):\n",
    "        conv_cache, relu_cache, pool_cache = caches[l]\n",
    "        \n",
    "        # Pooling\n",
    "        dA = pool_backward(dA, pool_cache, mode=\"max\")\n",
    "        \n",
    "        # ReLU\n",
    "        dZ = relu_backward(dA, relu_cache)\n",
    "        \n",
    "        # Faltung\n",
    "        dA, dW, db = conv_backward(dZ, conv_cache)\n",
    "        \n",
    "        gradients[f'dW{l+1}'] = dW\n",
    "        gradients[f'db{l+1}'] = db\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aktualisierung der Parameter und Vorhersage\n",
    "\n",
    "Wir implementieren Funktionen zur Aktualisierung der Parameter und zur Vorhersage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    \"\"\"\n",
    "    Aktualisiert die Parameter des Modells.\n",
    "    \n",
    "    Args:\n",
    "        parameters: Dictionary mit den Parametern des Modells\n",
    "        gradients: Dictionary mit den Gradienten\n",
    "        learning_rate: Lernrate\n",
    "        \n",
    "    Returns:\n",
    "        parameters: Aktualisierte Parameter\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2 - 1  # Anzahl der Faltungsschichten\n",
    "    \n",
    "    # Aktualisierung der Parameter der Faltungsschichten\n",
    "    for l in range(1, L + 1):\n",
    "        parameters[f'W{l}'] -= learning_rate * gradients[f'dW{l}']\n",
    "        parameters[f'b{l}'] -= learning_rate * gradients[f'db{l}']\n",
    "    \n",
    "    # Aktualisierung der Parameter der Fully-Connected-Schicht\n",
    "    parameters['W_fc'] -= learning_rate * gradients['dW_fc']\n",
    "    parameters['b_fc'] -= learning_rate * gradients['db_fc']\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def predict(X, parameters):\n",
    "    \"\"\"\n",
    "    Macht Vorhersagen mit dem trainierten Modell.\n",
    "    \n",
    "    Args:\n",
    "        X: Eingabedaten\n",
    "        parameters: Trainierte Parameter des Modells\n",
    "        \n",
    "    Returns:\n",
    "        predictions: Vorhersagen (0 oder 1)\n",
    "    \"\"\"\n",
    "    AL, _ = model_forward(X, parameters)\n",
    "    predictions = (AL > 0.5).astype(int)\n",
    "    return predictions\n",
    "\n",
    "def compute_accuracy(predictions, Y):\n",
    "    \"\"\"\n",
    "    Berechnet die Genauigkeit der Vorhersagen.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Vorhersagen (0 oder 1)\n",
    "        Y: Tatsächliche Labels\n",
    "        \n",
    "    Returns:\n",
    "        accuracy: Genauigkeit\n",
    "    \"\"\"\n",
    "    return np.mean(predictions == Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Batch Gradient Descent\n",
    "\n",
    "Wir implementieren Mini-Batch Gradient Descent für das Training des Modells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batches(X, Y, batch_size):\n",
    "    \"\"\"\n",
    "    Erstellt Mini-Batches für das Training.\n",
    "    \n",
    "    Args:\n",
    "        X: Eingabedaten\n",
    "        Y: Labels\n",
    "        batch_size: Größe der Mini-Batches\n",
    "        \n",
    "    Returns:\n",
    "        mini_batches: Liste von Mini-Batches\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    mini_batches = []\n",
    "    \n",
    "    # Mischen der Daten\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation, :, :, :]\n",
    "    shuffled_Y = Y[permutation, :]\n",
    "    \n",
    "    # Erstellen der Mini-Batches\n",
    "    num_complete_minibatches = m // batch_size\n",
    "    \n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * batch_size:(k + 1) * batch_size, :, :, :]\n",
    "        mini_batch_Y = shuffled_Y[k * batch_size:(k + 1) * batch_size, :]\n",
    "        mini_batches.append((mini_batch_X, mini_batch_Y))\n",
    "    \n",
    "    # Letzter Mini-Batch\n",
    "    if m % batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches * batch_size:, :, :, :]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches * batch_size:, :]\n",
    "        mini_batches.append((mini_batch_X, mini_batch_Y))\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hauptfunktion für das Training des Modells\n",
    "\n",
    "Wir implementieren die Hauptfunktion für das Training des Modells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate=0.01, num_epochs=10, batch_size=32, print_cost=True):\n",
    "    \"\"\"\n",
    "    Trainiert ein CNN-Modell.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Trainingsdaten\n",
    "        Y_train: Trainingslabels\n",
    "        X_test: Testdaten\n",
    "        Y_test: Testlabels\n",
    "        learning_rate: Lernrate\n",
    "        num_epochs: Anzahl der Epochen\n",
    "        batch_size: Größe der Mini-Batches\n",
    "        print_cost: Ob die Kosten ausgegeben werden sollen\n",
    "        \n",
    "    Returns:\n",
    "        parameters: Trainierte Parameter des Modells\n",
    "        costs: Liste der Kosten während des Trainings\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    costs = []\n",
    "    \n",
    "    # Initialisierung der Parameter\n",
    "    parameters = initialize_parameters([3, 3, 3], [8, 16, 32])\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_cost = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Mini-Batches erstellen\n",
    "        minibatches = mini_batches(X_train, Y_train, batch_size)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for minibatch in minibatches:\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "            \n",
    "            # Vorwärtsdurchlauf\n",
    "            AL, caches = model_forward(minibatch_X, parameters)\n",
    "            \n",
    "            # Kosten berechnen\n",
    "            cost = compute_cost(AL, minibatch_Y)\n",
    "            epoch_cost += cost\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Rückwärtsdurchlauf\n",
    "            gradients = model_backward(AL, minibatch_Y, caches)\n",
    "            \n",
    "            # Parameter aktualisieren\n",
    "            parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "        \n",
    "        epoch_cost /= num_batches\n",
    "        costs.append(epoch_cost)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "        \n",
    "        if print_cost and epoch % 1 == 0:\n",
    "            print(f\"Kosten nach Epoche {epoch}: {epoch_cost:.4f} (Zeit: {epoch_time:.2f}s)\")\n",
    "            \n",
    "            # Genauigkeit auf dem Trainingsdatensatz\n",
    "            train_predictions = predict(X_train, parameters)\n",
    "            train_accuracy = compute_accuracy(train_predictions, Y_train)\n",
    "            print(f\"Trainingsgenauigkeit: {train_accuracy:.4f}\")\n",
    "            \n",
    "            # Genauigkeit auf dem Testdatensatz\n",
    "            test_predictions = predict(X_test, parameters)\n",
    "            test_accuracy = compute_accuracy(test_predictions, Y_test)\n",
    "            print(f\"Testgenauigkeit: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Speichern der Parameter\n",
    "    np.save(os.path.join(models_dir, 'custom_cnn', 'custom_cnn_parameters.npy'), parameters)\n",
    "    print(f\"Parameter wurden gespeichert unter: {os.path.join(models_dir, 'custom_cnn','custom_cnn_parameters.npy')}\")\n",
    "    \n",
    "    # Visualisierung der Kosten\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(costs)\n",
    "    plt.title('Kosten während des Trainings')\n",
    "    plt.xlabel('Epochen')\n",
    "    plt.ylabel('Kosten')\n",
    "    plt.savefig(os.path.join(models_dir, 'custom_cnn_costs.png'))\n",
    "    print(f\"Kostenverlauf wurde gespeichert unter: {os.path.join(models_dir, 'custom_cnn_costs.png')}\")\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training des Modells\n",
    "\n",
    "Wir trainieren das Modell mit den reduzierten Daten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training des Modells\n",
    "print(\"Training des benutzerdefinierten CNN-Modells...\")\n",
    "parameters, costs = model(x_train_small, y_train_small, x_test_small, y_test_small, \n",
    "                         learning_rate=0.01, num_epochs=5, batch_size=32, print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluierung des Modells\n",
    "\n",
    "Wir evaluieren das trainierte Modell auf den Testdaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluierung des Modells auf den Testdaten\n",
    "print(\"Evaluierung des Modells auf den Testdaten...\")\n",
    "test_predictions = predict(x_test_small, parameters)\n",
    "test_accuracy = compute_accuracy(test_predictions, y_test_small)\n",
    "print(f\"Testgenauigkeit: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisierung der Vorhersagen\n",
    "\n",
    "Wir visualisieren einige Vorhersagen des Modells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung einiger Vorhersagen\n",
    "def plot_predictions(X, Y, predictions, num_images=25):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(min(num_images, len(X))):\n",
    "        plt.subplot(5, 5, i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(X[i])\n",
    "        \n",
    "        predicted = \"Auto\" if predictions[i] == 1 else \"Nicht-Auto\"\n",
    "        actual = \"Auto\" if Y[i] == 1 else \"Nicht-Auto\"\n",
    "        \n",
    "        color = 'green' if predicted == actual else 'red'\n",
    "        plt.xlabel(f\"P: {predicted}, A: {actual}\", color=color)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(models_dir, 'custom_cnn_predictions.png'))\n",
    "    print(f\"Vorhersagebeispiele wurden gespeichert unter: {os.path.join(models_dir, 'custom_cnn_predictions.png')}\")\n",
    "\n",
    "# Zufällige Auswahl von Testbildern\n",
    "np.random.seed(42)\n",
    "random_indices = np.random.choice(len(x_test_small), 25, replace=False)\n",
    "plot_predictions(\n",
    "    x_test_small[random_indices],\n",
    "    y_test_small[random_indices],\n",
    "    test_predictions[random_indices]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zusammenfassung\n",
    "\n",
    "In diesem Notebook haben wir:\n",
    "1. Ein CNN von Grund auf mit NumPy implementiert, ohne keras.models oder keras.layers zu verwenden\n",
    "2. Die Vorwärts- und Rückwärtspropagierung für alle Schichten implementiert\n",
    "3. Das Modell mit Mini-Batch Gradient Descent trainiert\n",
    "4. Das Modell auf Testdaten evaluiert\n",
    "5. Die Ergebnisse visualisiert\n",
    "\n",
    "Diese Implementierung zeigt, wie ein CNN von Grund auf funktioniert, ohne auf die Abstraktionen von Keras oder TensorFlow zurückzugreifen. Dies gibt uns ein tieferes Verständnis der zugrunde liegenden Mechanismen von CNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Benutzerdefiniertes CNN-Modell ohne keras.models oder keras.layers wurde erfolgreich trainiert und evaluiert.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
