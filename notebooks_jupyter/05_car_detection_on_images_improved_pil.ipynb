{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verbesserte Automerkennung auf Bildern (PIL-Version)\n",
    "\n",
    "Dieses Notebook implementiert eine verbesserte Automerkennung auf Bildern mit dem trainierten CNN-Modell. Es verwendet einen kombinierten Ansatz mit Selective Search für Region Proposals und Multi-Scale-Erkennung, um Autos in Bildern zuverlässiger zu lokalisieren und zu markieren. Diese Version verwendet PIL anstelle von OpenCV für die Bildverarbeitung, um Kompatibilität mit Python 3.11 zu gewährleisten.\n",
    "\n",
    "## Überblick\n",
    "- Laden des trainierten CNN-Modells\n",
    "- Implementierung eines alternativen Algorithmus für Region Proposals\n",
    "- Verbesserte Multi-Scale-Erkennung für verschiedene Objektgrößen\n",
    "- Optimierte Non-Maximum Suppression zur Entfernung überlappender Bounding Boxes\n",
    "- Anwendung auf Testbilder und Visualisierung der Ergebnisse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importieren der benötigten Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import time\n",
    "from skimage.feature import hog\n",
    "from skimage import exposure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorbereitung der Verzeichnisse\n",
    "\n",
    "Wir erstellen die notwendigen Verzeichnisse für Bilder und Ergebnisse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verzeichnisse\n",
    "models_dir = '../models'\n",
    "data_dir = '../data'\n",
    "images_dir = '../images'\n",
    "results_dir = '../results'\n",
    "\n",
    "os.makedirs(images_dir, exist_ok=True)\n",
    "os.makedirs(results_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laden des trainierten Modells\n",
    "\n",
    "Wir laden das in den vorherigen Notebooks trainierte CNN-Modell zur Autoerkennung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden des trainierten Modells\n",
    "print(\"Laden des trainierten Modells...\")\n",
    "try:\n",
    "    model = load_model(os.path.join(models_dir, 'keras_cnn', 'car_detection_model.keras'))\n",
    "    print(\"Modell erfolgreich geladen.\")\n",
    "except:\n",
    "    print(\"Fehler beim Laden des Modells. Bitte stellen Sie sicher, dass das Modell trainiert wurde.\")\n",
    "    # In einem Notebook verwenden wir keinen exit(1), sondern werfen eine Exception\n",
    "    raise Exception(\"Modell konnte nicht geladen werden.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funktionen für die Bildverarbeitung und Objekterkennung\n",
    "\n",
    "### Laden und Vorverarbeiten von Bildern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(image_path, target_size=(32, 32)):\n",
    "    \"\"\"\n",
    "    Lädt ein Bild und bereitet es für die Vorhersage vor.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Pfad zum Bild oder URL\n",
    "        target_size: Zielgröße für das Modell\n",
    "        \n",
    "    Returns:\n",
    "        image: Originalbild\n",
    "        processed_image: Vorverarbeitetes Bild für das Modell\n",
    "        (original_height, original_width): Originalgröße des Bildes\n",
    "    \"\"\"\n",
    "    # Überprüfen, ob es sich um eine URL handelt\n",
    "    if image_path.startswith('http'):\n",
    "        response = requests.get(image_path)\n",
    "        pil_image = Image.open(BytesIO(response.content))\n",
    "    else:\n",
    "        pil_image = Image.open(image_path)\n",
    "    \n",
    "    # Convert PIL image to numpy array\n",
    "    image = np.array(pil_image)\n",
    "    \n",
    "    # Speichern der Originalgröße\n",
    "    original_height, original_width = image.shape[:2]\n",
    "    \n",
    "    # Vorverarbeitung für das Modell\n",
    "    pil_resized = pil_image.resize(target_size)\n",
    "    processed_image = np.array(pil_resized).astype('float32') / 255.0\n",
    "    \n",
    "    return image, processed_image, (original_height, original_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative zu Selective Search für Region Proposals\n",
    "\n",
    "Da wir OpenCV nicht verwenden, implementieren wir eine alternative Methode für Region Proposals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_search_regions_alternative(image, method='fast'):\n",
    "    \"\"\"\n",
    "    Implementiert eine alternative zu Selective Search für Region Proposals.\n",
    "    \n",
    "    Args:\n",
    "        image: Eingabebild\n",
    "        method: 'fast' oder 'quality' für die Anzahl der generierten Regionen\n",
    "        \n",
    "    Returns:\n",
    "        regions: Liste der vorgeschlagenen Regionen (x, y, w, h)\n",
    "    \"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    regions = []\n",
    "    \n",
    "    # Verschiedene Fenstergrößen und Schrittweiten basierend auf der Methode\n",
    "    if method == 'fast':\n",
    "        window_sizes = [(64, 64), (96, 96), (128, 128), (196, 196), (256, 256)]\n",
    "        strides = [32, 48, 64, 98, 128]\n",
    "    else:  # 'quality'\n",
    "        window_sizes = [(64, 64), (96, 96), (128, 128), (160, 160), (196, 196), (224, 224), (256, 256), (320, 320)]\n",
    "        strides = [16, 24, 32, 48, 64, 80, 96, 128]\n",
    "    \n",
    "    # Generieren von Regionen mit verschiedenen Fenstergrößen und Schrittweiten\n",
    "    for window_size, stride in zip(window_sizes, strides):\n",
    "        for y in range(0, height - window_size[1], stride):\n",
    "            for x in range(0, width - window_size[0], stride):\n",
    "                regions.append((x, y, window_size[0], window_size[1]))\n",
    "    \n",
    "    # Zusätzliche Regionen für verschiedene Seitenverhältnisse\n",
    "    aspect_ratios = [0.5, 0.75, 1.0, 1.5, 2.0]\n",
    "    base_sizes = [64, 96, 128, 196, 256]\n",
    "    \n",
    "    for base_size in base_sizes:\n",
    "        for ratio in aspect_ratios:\n",
    "            w = int(base_size * ratio)\n",
    "            h = int(base_size / ratio)\n",
    "            stride = base_size // 2\n",
    "            \n",
    "            if w <= width and h <= height:\n",
    "                for y in range(0, height - h, stride):\n",
    "                    for x in range(0, width - w, stride):\n",
    "                        regions.append((x, y, w, h))\n",
    "    \n",
    "    # Filtern der Regionen nach Größe\n",
    "    min_area = 500  # Minimale Fläche für eine Region\n",
    "    max_area = image.shape[0] * image.shape[1] * 0.8  # Maximale Fläche (80% des Bildes)\n",
    "    \n",
    "    filtered_regions = []\n",
    "    for x, y, w, h in regions:\n",
    "        area = w * h\n",
    "        if min_area <= area <= max_area and w/h >= 0.5 and w/h <= 2.0:  # Verhältnis von Breite zu Höhe\n",
    "            filtered_regions.append((x, y, w, h))\n",
    "    \n",
    "    # Entfernen von Duplikaten\n",
    "    filtered_regions = list(set(filtered_regions))\n",
    "    \n",
    "    # Begrenzen der Anzahl der Regionen, um die Verarbeitung zu beschleunigen\n",
    "    max_regions = 300 if method == 'fast' else 500\n",
    "    if len(filtered_regions) > max_regions:\n",
    "        # Sortieren nach Größe (absteigend) und Auswahl der größten Regionen\n",
    "        filtered_regions.sort(key=lambda r: r[2] * r[3], reverse=True)\n",
    "        filtered_regions = filtered_regions[:max_regions]\n",
    "    \n",
    "    return filtered_regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HOG-Feature Extraktion\n",
    "\n",
    "Wir verwenden HOG (Histogram of Oriented Gradients) Features, um die Erkennung von Autos zu verbessern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hog_features(image, target_size=(64, 64)):\n",
    "    \"\"\"\n",
    "    Extrahiert HOG-Features aus einem Bild.\n",
    "    \n",
    "    Args:\n",
    "        image: Eingabebild\n",
    "        target_size: Zielgröße für die HOG-Feature-Extraktion\n",
    "        \n",
    "    Returns:\n",
    "        hog_image: Visualisierung der HOG-Features\n",
    "    \"\"\"\n",
    "    # Konvertieren in Graustufen\n",
    "    if len(image.shape) == 3:\n",
    "        # Verwenden von PIL für die Konvertierung in Graustufen\n",
    "        pil_image = Image.fromarray(image)\n",
    "        gray = np.array(pil_image.convert('L'))\n",
    "    else:\n",
    "        gray = image\n",
    "    \n",
    "    # Skalieren auf die Zielgröße\n",
    "    pil_gray = Image.fromarray(gray)\n",
    "    resized = np.array(pil_gray.resize(target_size))\n",
    "    \n",
    "    # HOG-Features extrahieren\n",
    "    features, hog_image = hog(resized, orientations=9, pixels_per_cell=(8, 8),\n",
    "                             cells_per_block=(2, 2), visualize=True, feature_vector=True)\n",
    "    \n",
    "    # Normalisieren für die Visualisierung\n",
    "    hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
    "    \n",
    "    return hog_image_rescaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verbesserte Objekterkennung mit Region Proposals\n",
    "\n",
    "Diese Funktion kombiniert unsere alternative Region Proposal Methode mit dem CNN-Modell zur Klassifikation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_cars_with_region_proposals(image, model, confidence_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Erkennt Autos in einem Bild mit alternativen Region Proposals.\n",
    "    \n",
    "    Args:\n",
    "        image: Eingabebild\n",
    "        model: Trainiertes Modell\n",
    "        confidence_threshold: Schwellenwert für die Konfidenz\n",
    "        \n",
    "    Returns:\n",
    "        detections: Liste der erkannten Autos (x, y, w, h, confidence)\n",
    "    \"\"\"\n",
    "    # Regionen mit alternativer Methode vorschlagen\n",
    "    regions = selective_search_regions_alternative(image)\n",
    "    \n",
    "    detections = []\n",
    "    \n",
    "    for x, y, w, h in regions:\n",
    "        # Extrahieren der Region\n",
    "        if y + h <= image.shape[0] and x + w <= image.shape[1]:  # Sicherstellen, dass die Region innerhalb des Bildes liegt\n",
    "            region = image[y:y+h, x:x+w]\n",
    "            \n",
    "            # Vorverarbeitung der Region\n",
    "            try:\n",
    "                pil_region = Image.fromarray(region)\n",
    "                region_resized = np.array(pil_region.resize((32, 32)))\n",
    "                region_normalized = region_resized.astype('float32') / 255.0\n",
    "                region_batch = np.expand_dims(region_normalized, axis=0)\n",
    "                \n",
    "                # Vorhersage\n",
    "                prediction = model.predict(region_batch, verbose=0)[0][0]\n",
    "                \n",
    "                # Wenn die Konfidenz über dem Schwellenwert liegt, speichern wir die Erkennung\n",
    "                if prediction > confidence_threshold:\n",
    "                    detections.append((x, y, w, h, prediction))\n",
    "            except Exception as e:\n",
    "                # Ignorieren von Regionen, die nicht verarbeitet werden können\n",
    "                continue\n",
    "    \n",
    "    return detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verbesserte Multi-Scale-Erkennung\n",
    "\n",
    "Diese Funktion kombiniert Region Proposals mit Multi-Scale-Erkennung für eine verbesserte Automerkennung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_cars_multi_scale_improved(image, model, scales=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0, 2.5], \n",
    "                                    confidence_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Erkennt Autos in einem Bild mit verbesserter Multi-Scale-Erkennung.\n",
    "    \n",
    "    Args:\n",
    "        image: Eingabebild\n",
    "        model: Trainiertes Modell\n",
    "        scales: Liste der Skalierungsfaktoren\n",
    "        confidence_threshold: Schwellenwert für die Konfidenz\n",
    "        \n",
    "    Returns:\n",
    "        detections: Liste der erkannten Autos (x, y, w, h, confidence)\n",
    "    \"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    detections = []\n",
    "    \n",
    "    # Erkennung mit Region Proposals auf dem Originalbild\n",
    "    detections.extend(detect_cars_with_region_proposals(image, model, confidence_threshold))\n",
    "    \n",
    "    # Multi-Scale-Erkennung\n",
    "    for scale in scales:\n",
    "        # Skalieren des Bildes\n",
    "        scaled_height = int(height * scale)\n",
    "        scaled_width = int(width * scale)\n",
    "        \n",
    "        # Verwenden von PIL für die Skalierung\n",
    "        pil_image = Image.fromarray(image)\n",
    "        scaled_image = np.array(pil_image.resize((scaled_width, scaled_height)))\n",
    "        \n",
    "        # Erkennen von Autos im skalierten Bild mit Region Proposals\n",
    "        scaled_detections = detect_cars_with_region_proposals(scaled_image, model, confidence_threshold)\n",
    "        \n",
    "        # Anpassen der Koordinaten an die Originalgröße\n",
    "        for (x, y, w, h, conf) in scaled_detections:\n",
    "            x_orig = int(x / scale)\n",
    "            y_orig = int(y / scale)\n",
    "            w_orig = int(w / scale)\n",
    "            h_orig = int(h / scale)\n",
    "            detections.append((x_orig, y_orig, w_orig, h_orig, conf))\n",
    "    \n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_large_cars(image, model, confidence_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Erkennt große Autos, die einen signifikanten Teil des Bildes einnehmen.\n",
    "    \n",
    "    Args:\n",
    "        image: Eingabebild\n",
    "        model: Trainiertes Modell\n",
    "        confidence_threshold: Schwellenwert für die Konfidenz\n",
    "        \n",
    "    Returns:\n",
    "        detections: Liste der erkannten großen Autos (x, y, w, h, confidence)\n",
    "    \"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    detections = []\n",
    "    \n",
    "    # Definieren von großen Regionen, die signifikante Teile des Bildes abdecken\n",
    "    # Wir erstellen Regionen, die 40% bis 90% des Bildes abdecken\n",
    "    coverage_ratios = [0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    \n",
    "    for ratio in coverage_ratios:\n",
    "        # Berechnen der Größe der Region basierend auf dem Verhältnis\n",
    "        region_width = int(width * ratio)\n",
    "        region_height = int(height * ratio)\n",
    "        \n",
    "        # Berechnen der Position der Region (zentriert)\n",
    "        x = (width - region_width) // 2\n",
    "        y = (height - region_height) // 2\n",
    "        \n",
    "        # Extrahieren der Region\n",
    "        region = image[y:y+region_height, x:x+region_width]\n",
    "        \n",
    "        # Vorverarbeitung der Region\n",
    "        try:\n",
    "            pil_region = Image.fromarray(region)\n",
    "            region_resized = np.array(pil_region.resize((32, 32)))\n",
    "            region_normalized = region_resized.astype('float32') / 255.0\n",
    "            region_batch = np.expand_dims(region_normalized, axis=0)\n",
    "            \n",
    "            # Vorhersage\n",
    "            prediction = model.predict(region_batch, verbose=0)[0][0]\n",
    "            \n",
    "            # Wenn die Konfidenz über dem Schwellenwert liegt, speichern wir die Erkennung\n",
    "            if prediction > confidence_threshold:\n",
    "                detections.append((x, y, region_width, region_height, prediction))\n",
    "        except Exception as e:\n",
    "            # Ignorieren von Regionen, die nicht verarbeitet werden können\n",
    "            continue\n",
    "    \n",
    "    return detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erweiterte Non-Maximum Suppression\n",
    "\n",
    "Diese Funktion implementiert eine erweiterte Non-Maximum Suppression, die 100% überlappende Boxen entfernt und eine intelligente Filterung für verschachtelte Boxen bietet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_non_max_suppression(boxes, overlap_threshold=0.3, score_threshold=0.6, containment_threshold=0.95):\n",
    "    \"\"\"\n",
    "    Führt eine erweiterte Non-Maximum Suppression durch, die 100% überlappende Boxen entfernt\n",
    "    und eine intelligente Filterung für verschachtelte Boxen implementiert.\n",
    "    \n",
    "    Args:\n",
    "        boxes: Liste der Bounding Boxes (x, y, w, h, confidence)\n",
    "        overlap_threshold: Schwellenwert für die Überlappung (IoU)\n",
    "        score_threshold: Minimaler Konfidenzwert\n",
    "        containment_threshold: Schwellenwert für die Enthaltung einer Box in einer anderen\n",
    "        \n",
    "    Returns:\n",
    "        picked: Liste der ausgewählten Bounding Boxes\n",
    "    \"\"\"\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Filtern nach Konfidenz\n",
    "    boxes = [box for box in boxes if box[4] >= score_threshold]\n",
    "    \n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Konvertieren der Bounding Boxes in das Format (x1, y1, x2, y2, conf)\n",
    "    boxes_array = np.array([(x, y, x + w, y + h, conf) for x, y, w, h, conf in boxes])\n",
    "    \n",
    "    # Sortieren der Bounding Boxes nach Konfidenz (absteigend)\n",
    "    boxes_array = boxes_array[np.argsort(boxes_array[:, 4])[::-1]]\n",
    "    \n",
    "    # Entfernen von 100% überlappenden Boxen (Duplikate)\n",
    "    unique_boxes = []\n",
    "    for box in boxes_array:\n",
    "        is_duplicate = False\n",
    "        for unique_box in unique_boxes:\n",
    "            if (box[0] == unique_box[0] and box[1] == unique_box[1] and \n",
    "                box[2] == unique_box[2] and box[3] == unique_box[3]):\n",
    "                is_duplicate = True\n",
    "                break\n",
    "        if not is_duplicate:\n",
    "            unique_boxes.append(box)\n",
    "    \n",
    "    boxes_array = np.array(unique_boxes)\n",
    "    \n",
    "    picked = []\n",
    "    \n",
    "    while len(boxes_array) > 0:\n",
    "        # Die Box mit der höchsten Konfidenz auswählen\n",
    "        current_box = boxes_array[0]\n",
    "        picked.append(current_box)\n",
    "        \n",
    "        # Berechnen der Überlappung mit den verbleibenden Boxen\n",
    "        remaining_boxes = boxes_array[1:]\n",
    "        \n",
    "        if len(remaining_boxes) == 0:\n",
    "            break\n",
    "        \n",
    "        # Berechnen der Koordinaten der Überlappung\n",
    "        xx1 = np.maximum(current_box[0], remaining_boxes[:, 0])\n",
    "        yy1 = np.maximum(current_box[1], remaining_boxes[:, 1])\n",
    "        xx2 = np.minimum(current_box[2], remaining_boxes[:, 2])\n",
    "        yy2 = np.minimum(current_box[3], remaining_boxes[:, 3])\n",
    "        \n",
    "        # Berechnen der Breite und Höhe der Überlappung\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    "        \n",
    "        # Berechnen des Überlappungsverhältnisses (IoU - Intersection over Union)\n",
    "        intersection = w * h\n",
    "        area1 = (current_box[2] - current_box[0] + 1) * (current_box[3] - current_box[1] + 1)\n",
    "        area2 = (remaining_boxes[:, 2] - remaining_boxes[:, 0] + 1) * (remaining_boxes[:, 3] - remaining_boxes[:, 1] + 1)\n",
    "        union = area1 + area2 - intersection\n",
    "        iou = intersection / union\n",
    "        \n",
    "        # Berechnen des Verhältnisses der Überlappung zur Fläche der kleineren Box\n",
    "        # Dies hilft zu erkennen, ob eine Box fast vollständig in einer anderen enthalten ist\n",
    "        containment_ratio1 = intersection / area1  # Wie viel von Box 1 ist in der Überlappung enthalten\n",
    "        containment_ratio2 = intersection / area2  # Wie viel von Box 2 ist in der Überlappung enthalten\n",
    "        \n",
    "        # Indizes der Boxen, die entfernt werden sollen\n",
    "        to_remove = []\n",
    "        \n",
    "        for i in range(len(remaining_boxes)):\n",
    "            # Wenn die IoU über dem Schwellenwert liegt, entfernen wir die Box\n",
    "            if iou[i] > overlap_threshold:\n",
    "                to_remove.append(i)\n",
    "            # Wenn eine Box fast vollständig in der aktuellen Box enthalten ist\n",
    "            elif containment_ratio2[i] > containment_threshold:\n",
    "                # Wenn die kleinere Box eine höhere Konfidenz hat, behalten wir sie\n",
    "                if remaining_boxes[i, 4] > current_box[4] * 1.2:  # 20% höhere Konfidenz\n",
    "                    continue\n",
    "                to_remove.append(i)\n",
    "        \n",
    "        # Erstellen einer Maske für die zu behaltenden Boxen\n",
    "        mask = np.ones(len(remaining_boxes), dtype=bool)\n",
    "        mask[to_remove] = False\n",
    "        \n",
    "        # Aktualisieren der verbleibenden Boxen\n",
    "        boxes_array = remaining_boxes[mask]\n",
    "    \n",
    "    # Konvertieren zurück in das Format (x, y, w, h, confidence)\n",
    "    picked = [(box[0], box[1], box[2] - box[0], box[3] - box[1], box[4]) for box in picked]\n",
    "    \n",
    "    return picked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zeichnen der Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(image, boxes):\n",
    "    \"\"\"\n",
    "    Zeichnet Bounding Boxes auf ein Bild.\n",
    "    \n",
    "    Args:\n",
    "        image: Eingabebild\n",
    "        boxes: Liste der Bounding Boxes (x, y, w, h, confidence)\n",
    "        \n",
    "    Returns:\n",
    "        result: Bild mit Bounding Boxes\n",
    "    \"\"\"\n",
    "    # Convert numpy array to PIL Image\n",
    "    pil_image = Image.fromarray(image)\n",
    "    draw = ImageDraw.Draw(pil_image)\n",
    "    \n",
    "    for (x, y, w, h, conf) in boxes:\n",
    "        # Convert coordinates to integers\n",
    "        x, y, w, h = int(x), int(y), int(w), int(h)\n",
    "        \n",
    "        # Draw rectangle\n",
    "        draw.rectangle([(x, y), (x + w, y + h)], outline=(0, 255, 0), width=2)\n",
    "        \n",
    "        # Draw confidence text\n",
    "        label = f\"Car: {conf:.2f}\"\n",
    "        draw.text((x, y - 10), label, fill=(0, 255, 0))\n",
    "    \n",
    "    # Convert back to numpy array\n",
    "    result = np.array(pil_image)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hauptfunktion zur verbesserten Erkennung von Autos in Bildern\n",
    "\n",
    "Diese Funktion kombiniert alle vorherigen Funktionen, um Autos in einem Bild zu erkennen und zu markieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_draw_cars_improved(image_path, model, output_path):\n",
    "    \"\"\"\n",
    "    Erkennt Autos in einem Bild mit verbesserter Methode und zeichnet Bounding Boxes.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Pfad zum Bild oder URL\n",
    "        model: Trainiertes Modell\n",
    "        output_path: Pfad zum Ausgabebild\n",
    "        \n",
    "    Returns:\n",
    "        boxes: Liste der erkannten Autos (x, y, w, h, confidence)\n",
    "    \"\"\"\n",
    "    # Laden und Vorverarbeiten des Bildes\n",
    "    image, processed_image, (original_height, original_width) = load_and_preprocess_image(image_path)\n",
    "    \n",
    "    # Zeitmessung starten\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Erkennen von Autos im Bild mit verbesserter Multi-Scale-Erkennung\n",
    "    boxes = detect_cars_multi_scale_improved(image, model, confidence_threshold=0.6)\n",
    "    \n",
    "    # Erkennen von großen Autos, die einen signifikanten Teil des Bildes einnehmen\n",
    "    large_car_boxes = detect_large_cars(image, model, confidence_threshold=0.6)\n",
    "    boxes.extend(large_car_boxes)\n",
    "    \n",
    "    # Zusammenführen überlappender Bounding Boxes mit erweiterter NMS\n",
    "    boxes = advanced_non_max_suppression(boxes, overlap_threshold=0.3, score_threshold=0.6, containment_threshold=0.95)\n",
    "    \n",
    "    # Zeitmessung beenden\n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    \n",
    "    # Zeichnen der Bounding Boxes\n",
    "    result = draw_boxes(image, boxes)\n",
    "    \n",
    "    # Hinzufügen von Informationen zum Bild\n",
    "    pil_result = Image.fromarray(result)\n",
    "    draw = ImageDraw.Draw(pil_result)\n",
    "    info_text = f\"Erkannte Autos: {len(boxes)} | Verarbeitungszeit: {processing_time:.2f}s\"\n",
    "    draw.text((10, 30), info_text, fill=(0, 0, 255))\n",
    "    result = np.array(pil_result)\n",
    "    \n",
    "    # Speichern des Ergebnisses\n",
    "    Image.fromarray(result).save(output_path)\n",
    "    \n",
    "    # Erstellen einzelner Bilder für jedes erkannte Auto\n",
    "    for i, (x, y, w, h, conf) in enumerate(boxes):\n",
    "        # Convert coordinates to integers\n",
    "        x, y, w, h = int(x), int(y), int(w), int(h)\n",
    "        \n",
    "        # Extrahieren des Autos\n",
    "        car_image = image[y:y+h, x:x+w]\n",
    "        \n",
    "        # Zeichnen einer Box um das Auto\n",
    "        pil_car = Image.fromarray(car_image)\n",
    "        draw = ImageDraw.Draw(pil_car)\n",
    "        draw.rectangle([(0, 0), (w, h)], outline=(0, 255, 0), width=2)\n",
    "        car_image_with_box = np.array(pil_car)\n",
    "        \n",
    "        # Speichern des Bildes\n",
    "        car_output_path = output_path.replace('.jpg', f'_car_{i+1}.jpg')\n",
    "        Image.fromarray(car_image_with_box).save(car_output_path)\n",
    "    \n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anwendung auf Testbilder\n",
    "\n",
    "### Testen der verbesserten Automerkennung auf den Bildern aus dem Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testen der verbesserten Automerkennung auf den Bildern aus dem Repository...\")\n",
    "\n",
    "# Bilder aus dem Repository\n",
    "repo_images = [\n",
    "    os.path.join(images_dir, 'bild1.jpg'),\n",
    "    os.path.join(images_dir, 'bild2.jpg'),\n",
    "    os.path.join(images_dir, 'bild3.jpg')\n",
    "]\n",
    "\n",
    "for i, image_path in enumerate(repo_images):\n",
    "    # Erkennen von Autos im Bild\n",
    "    output_path = os.path.join(results_dir, f'bild{i+1}_result_improved.jpg')\n",
    "    boxes = detect_and_draw_cars_improved(image_path, model, output_path)\n",
    "    \n",
    "    print(f\"Bild {i+1}: {len(boxes)} Autos erkannt\")\n",
    "    \n",
    "    # Anzeigen des Ergebnisses im Notebook\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(plt.imread(output_path))\n",
    "    plt.title(f\"Bild {i+1}: {len(boxes)} Autos erkannt\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testen der verbesserten Automerkennung auf zusätzlichen Bildern aus dem Internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testen der verbesserten Automerkennung auf zusätzlichen Bildern aus dem Internet...\")\n",
    "\n",
    "# Suchen nach Bildern mit mehreren Autos\n",
    "additional_images = [\n",
    "    \"https://cdn.pixabay.com/photo/2017/11/23/04/13/traffic-jam-2972156_1280.jpg\",\n",
    "    \"https://cdn.pixabay.com/photo/2017/08/01/09/34/car-2563902_1280.jpg\",\n",
    "    \"https://cdn.pixabay.com/photo/2017/08/06/12/08/car-2592150_1280.jpg\"\n",
    "]\n",
    "\n",
    "for i, image_url in enumerate(additional_images):\n",
    "    # Speichern des Bildes\n",
    "    image_path = os.path.join(images_dir, f'additional_image_{i+1}.jpg')\n",
    "    \n",
    "    # Herunterladen des Bildes, wenn es noch nicht existiert\n",
    "    if not os.path.exists(image_path):\n",
    "        response = requests.get(image_url)\n",
    "        with open(image_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "    \n",
    "    # Erkennen von Autos im Bild\n",
    "    output_path = os.path.join(results_dir, f'additional_image_{i+1}_result_improved.jpg')\n",
    "    boxes = detect_and_draw_cars_improved(image_path, model, output_path)\n",
    "    \n",
    "    print(f\"Zusätzliches Bild {i+1}: {len(boxes)} Autos erkannt\")\n",
    "    \n",
    "    # Anzeigen des Ergebnisses im Notebook\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(plt.imread(output_path))\n",
    "    plt.title(f\"Zusätzliches Bild {i+1}: {len(boxes)} Autos erkannt\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zusammenfassung\n",
    "\n",
    "In diesem Notebook haben wir:\n",
    "1. Ein trainiertes CNN-Modell zur Autoerkennung geladen\n",
    "2. Einen verbesserten Ansatz zur Automerkennung implementiert, der eine alternative zu Selective Search für Region Proposals verwendet\n",
    "3. Eine verbesserte Multi-Scale-Erkennung implementiert, um Autos unterschiedlicher Größen zuverlässiger zu erkennen\n",
    "4. Eine spezielle Erkennung für sehr große Autos implementiert, die einen signifikanten Teil des Bildes einnehmen\n",
    "5. Eine erweiterte Non-Maximum Suppression implementiert, die 100% überlappende Boxen entfernt und eine intelligente Filterung für verschachtelte Boxen bietet\n",
    "6. Die Erkennung auf verschiedenen Testbildern angewendet und die Ergebnisse visualisiert\n",
    "7. Alle Bildverarbeitungsfunktionen mit PIL anstelle von OpenCV implementiert, um Kompatibilität mit Python 3.11 zu gewährleisten\n",
    "\n",
    "Der implementierte verbesserte Ansatz ermöglicht eine zuverlässigere Erkennung von Autos in Bildern unterschiedlicher Größen und Perspektiven. Die Verwendung einer alternativen Methode für Region Proposals anstelle des Sliding-Window-Ansatzes führt zu einer besseren Erkennung von Autos, insbesondere in komplexen Szenen mit mehreren Autos. Die spezielle Erkennung für sehr große Autos ermöglicht es, Autos zu erkennen, die einen signifikanten Teil des Bildes einnehmen. Die erweiterte Non-Maximum Suppression verhindert 100% überlappende Boxen und bietet eine intelligente Filterung für verschachtelte Boxen, die nicht einfach kleinere Boxen innerhalb größerer Boxen entfernt, sondern die Konfidenz und das Enthaltensein berücksichtigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Verbesserte Automerkennung auf Bildern abgeschlossen. Die Ergebnisse wurden im Verzeichnis 'results' gespeichert.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
