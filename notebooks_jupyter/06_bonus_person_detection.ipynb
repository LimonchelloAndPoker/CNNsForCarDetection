{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: CNN zur Erkennung von Personen und Autos\n",
    "\n",
    "Dieses Notebook implementiert ein CNN zur Erkennung von Personen und Autos und wendet es auf Bilder an, die sowohl Personen als auch Autos enthalten.\n",
    "\n",
    "## Überblick\n",
    "- Laden und Vorbereiten des Datensatzes für Personenerkennung\n",
    "- Training eines CNN-Modells für Personenerkennung\n",
    "- Laden des vortrainierten Autoerkennungsmodells\n",
    "- Implementierung einer verbesserten Erkennungsmethode basierend auf dem 05-Notebook\n",
    "- Anwendung auf Testbilder mit Personen und Autos\n",
    "- Visualisierung der Ergebnisse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importieren der benötigten Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import time\n",
    "import glob\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorbereitung der Verzeichnisse\n",
    "\n",
    "Wir erstellen die notwendigen Verzeichnisse für Modelle, Bilder, Ergebnisse und Bonus-Ergebnisse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verzeichnisse\n",
    "models_dir = '../models'\n",
    "data_dir = '../data'\n",
    "images_dir = '../images'\n",
    "results_dir = '../results'\n",
    "bonus_dir = '../bonus'\n",
    "human_dataset_dir = '/home/ubuntu/HumanBinaryClassificationSuite/dataset'\n",
    "\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(models_dir, 'human_detection'), exist_ok=True)\n",
    "os.makedirs(images_dir, exist_ok=True)\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "os.makedirs(bonus_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laden und Vorbereiten des Datensatzes für Personenerkennung\n",
    "\n",
    "Wir laden den Datensatz aus dem HumanBinaryClassificationSuite Repository und bereiten ihn für das Training vor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_dataset(dataset_dir, target_size=(32, 32), batch_size=32, validation_split=0.2):\n",
    "    \"\"\"\n",
    "    Lädt den Datensatz und bereitet ihn für das Training vor.\n",
    "    \n",
    "    Args:\n",
    "        dataset_dir: Verzeichnis mit den Unterordnern 'human' und 'nonhuman'\n",
    "        target_size: Zielgröße für die Bilder\n",
    "        batch_size: Batch-Größe für das Training\n",
    "        validation_split: Anteil der Daten für die Validierung\n",
    "        \n",
    "    Returns:\n",
    "        train_generator: Generator für Trainingsdaten\n",
    "        validation_generator: Generator für Validierungsdaten\n",
    "        class_weights: Gewichtung der Klassen für unbalancierte Daten\n",
    "    \"\"\"\n",
    "    print(f\"Lade Datensatz aus {dataset_dir}...\")\n",
    "    \n",
    "    # Datenaugmentation für Trainingsdaten\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest',\n",
    "        validation_split=validation_split\n",
    "    )\n",
    "    \n",
    "    # Generator für Trainingsdaten\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        dataset_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary',\n",
    "        subset='training'\n",
    "    )\n",
    "    \n",
    "    # Generator für Validierungsdaten\n",
    "    validation_generator = train_datagen.flow_from_directory(\n",
    "        dataset_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary',\n",
    "        subset='validation'\n",
    "    )\n",
    "    \n",
    "    # Berechnen der Klassengewichte für unbalancierte Daten\n",
    "    human_count = len(os.listdir(os.path.join(dataset_dir, 'human')))\n",
    "    nonhuman_count = len(os.listdir(os.path.join(dataset_dir, 'nonhuman')))\n",
    "    total_count = human_count + nonhuman_count\n",
    "    \n",
    "    print(f\"Anzahl der Bilder: Human = {human_count}, Nonhuman = {nonhuman_count}, Gesamt = {total_count}\")\n",
    "    \n",
    "    # Klassengewichte berechnen (höheres Gewicht für die unterrepräsentierte Klasse)\n",
    "    weight_for_0 = (1 / nonhuman_count) * (total_count / 2.0)  # nonhuman\n",
    "    weight_for_1 = (1 / human_count) * (total_count / 2.0)     # human\n",
    "    \n",
    "    class_weights = {0: weight_for_0, 1: weight_for_1}\n",
    "    print(f\"Klassengewichte: {class_weights}\")\n",
    "    \n",
    "    return train_generator, validation_generator, class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden und Vorbereiten des Datensatzes\n",
    "train_generator, validation_generator, class_weights = load_and_prepare_dataset(\n",
    "    human_dataset_dir,\n",
    "    target_size=(32, 32),\n",
    "    batch_size=64,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition des CNN-Modells für Personenerkennung\n",
    "\n",
    "Wir definieren ein CNN-Modell zur Erkennung von Personen. Das Modell besteht aus mehreren Convolutional Blocks mit Batch Normalization und Dropout zur Regularisierung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_person_detection_model(input_shape=(32, 32, 3)):\n",
    "    model = Sequential([\n",
    "        # Erster Convolutional Block\n",
    "        Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Zweiter Convolutional Block\n",
    "        Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Dritter Convolutional Block\n",
    "        Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')  # Binäre Klassifikation: Person vs. Nicht-Person\n",
    "    ])\n",
    "    \n",
    "    # Kompilieren des Modells\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training des Personenerkennungsmodells\n",
    "\n",
    "Wir trainieren das Modell mit dem vorbereiteten Datensatz und speichern das beste Modell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen des Modells\n",
    "person_model = create_person_detection_model(input_shape=(32, 32, 3))\n",
    "\n",
    "# Zusammenfassung des Modells anzeigen\n",
    "person_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks für das Training\n",
    "checkpoint_path = os.path.join(models_dir, 'human_detection', 'human_detection_model.keras')\n",
    "checkpoint = ModelCheckpoint(\n",
    "    checkpoint_path,\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Training des Modells\n",
    "print(\"Training des Personenerkennungsmodells...\")\n",
    "history = person_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=30,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=len(validation_generator),\n",
    "    callbacks=[checkpoint, early_stopping],\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisierung des Trainingsverlaufs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung des Trainingsverlaufs\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training')\n",
    "plt.plot(history.history['val_accuracy'], label='Validierung')\n",
    "plt.title('Genauigkeit')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Genauigkeit')\n",
    "plt.legend()\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training')\n",
    "plt.plot(history.history['val_loss'], label='Validierung')\n",
    "plt.title('Verlust')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Verlust')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laden der trainierten Modelle\n",
    "\n",
    "Wir laden das trainierte Personenerkennungsmodell und das vorhandene Autoerkennungsmodell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden des trainierten Personenerkennungsmodells\n",
    "print(\"Laden des trainierten Personenerkennungsmodells...\")\n",
    "try:\n",
    "    person_model = load_model(os.path.join(models_dir, 'human_detection', 'human_detection_model.keras'))\n",
    "    print(\"Personenerkennungsmodell erfolgreich geladen.\")\n",
    "except:\n",
    "    print(\"Fehler beim Laden des Personenerkennungsmodells. Verwende das zuletzt trainierte Modell.\")\n",
    "\n",
    "# Laden des trainierten Auto-Modells\n",
    "print(\"Laden des trainierten Auto-Modells...\")\n",
    "try:\n",
    "    car_model = load_model(os.path.join(models_dir, 'keras_cnn', 'car_detection_model.keras'))\n",
    "    print(\"Auto-Modell erfolgreich geladen.\")\n",
    "except:\n",
    "    print(\"Fehler beim Laden des Auto-Modells. Bitte stellen Sie sicher, dass das Modell trainiert wurde.\")\n",
    "    raise Exception(\"Auto-Modell konnte nicht geladen werden.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funktionen für die Bildverarbeitung und Objekterkennung\n",
    "\n",
    "### Laden und Vorverarbeiten von Bildern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(image_path, target_size=(32, 32)):\n",
    "    \"\"\"\n",
    "    Lädt ein Bild und bereitet es für die Vorhersage vor.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Pfad zum Bild oder URL\n",
    "        target_size: Zielgröße für das Modell\n",
    "        \n",
    "    Returns:\n",
    "        image: Originalbild\n",
    "        processed_image: Vorverarbeitetes Bild für das Modell\n",
    "        (original_height, original_width): Originalgröße des Bildes\n",
    "    \"\"\"\n",
    "    # Überprüfen, ob es sich um eine URL handelt\n",
    "    if image_path.startswith('http'):\n",
    "        response = requests.get(image_path)\n",
    "        pil_image = Image.open(BytesIO(response.content))\n",
    "    else:\n",
    "        pil_image = Image.open(image_path)\n",
    "    \n",
    "    # Convert PIL image to numpy array\n",
    "    image = np.array(pil_image)\n",
    "    \n",
    "    # Speichern der Originalgröße\n",
    "    original_height, original_width = image.shape[:2]\n",
    "    \n",
    "    # Vorverarbeitung für das Modell\n",
    "    pil_resized = pil_image.resize(target_size)\n",
    "    processed_image = np.array(pil_resized).astype('float32') / 255.0\n",
    "    \n",
    "    return image, processed_image, (original_height, original_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region Proposals für Objekterkennung\n",
    "\n",
    "Wir implementieren eine Methode für Region Proposals, die auf dem 05-Notebook basiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_region_proposals(image, method='fast'):\n",
    "    \"\"\"\n",
    "    Generiert Regionsvorschläge für die Objekterkennung.\n",
    "    \n",
    "    Args:\n",
    "        image: Eingabebild\n",
    "        method: 'fast' oder 'quality' für die Anzahl der generierten Regionen\n",
    "        \n",
    "    Returns:\n",
    "        regions: Liste der vorgeschlagenen Regionen (x, y, w, h)\n",
    "    \"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    regions = []\n",
    "    \n",
    "    # Verschiedene Fenstergrößen und Schrittweiten basierend auf der Methode\n",
    "    if method == 'fast':\n",
    "        window_sizes = [(64, 64), (96, 96), (128, 128), (196, 196), (256, 256)]\n",
    "        strides = [32, 48, 64, 98, 128]\n",
    "    else:  # 'quality'\n",
    "        window_sizes = [(64, 64), (96, 96), (128, 128), (160, 160), (196, 196), (224, 224), (256, 256), (320, 320)]\n",
    "        strides = [16, 24, 32, 48, 64, 80, 96, 128]\n",
    "    \n",
    "    # Generieren von Regionen mit verschiedenen Fenstergrößen und Schrittweiten\n",
    "    for window_size, stride in zip(window_sizes, strides):\n",
    "        for y in range(0, height - window_size[1], stride):\n",
    "            for x in range(0, width - window_size[0], stride):\n",
    "                regions.append((x, y, window_size[0], window_size[1]))\n",
    "    \n",
    "    # Zusätzliche Regionen für verschiedene Seitenverhältnisse\n",
    "    aspect_ratios = [0.5, 0.75, 1.0, 1.5, 2.0]\n",
    "    base_sizes = [64, 96, 128, 196, 256]\n",
    "    \n",
    "    for base_size in base_sizes:\n",
    "        for ratio in aspect_ratios:\n",
    "            w = int(base_size * ratio)\n",
    "            h = int(base_size / ratio)\n",
    "            stride = base_size // 2\n",
    "            \n",
    "            if w <= width and h <= height:\n",
    "                for y in range(0, height - h, stride):\n",
    "                    for x in range(0, width - w, stride):\n",
    "                        regions.append((x, y, w, h))\n",
    "    \n",
    "    # Filtern der Regionen nach Größe\n",
    "    min_area = 500  # Minimale Fläche für eine Region\n",
    "    max_area = image.shape[0] * image.shape[1] * 0.8  # Maximale Fläche (80% des Bildes)\n",
    "    \n",
    "    filtered_regions = []\n",
    "    for x, y, w, h in regions:\n",
    "        area = w * h\n",
    "        if min_area <= area <= max_area and w/h >= 0.5 and w/h <= 2.0:  # Verhältnis von Breite zu Höhe\n",
    "            filtered_regions.append((x, y, w, h))\n",
    "    \n",
    "    # Entfernen von Duplikaten\n",
    "    filtered_regions = list(set(filtered_regions))\n",
    "    \n",
    "    # Begrenzen der Anzahl der Regionen, um die Verarbeitung zu beschleunigen\n",
    "    max_regions = 300 if method == 'fast' else 500\n",
    "    if len(filtered_regions) > max_regions:\n",
    "        # Sortieren nach Größe (absteigend) und Auswahl der größten Regionen\n",
    "        filtered_regions.sort(key=lambda r: r[2] * r[3], reverse=True)\n",
    "        filtered_regions = filtered_regions[:max_regions]\n",
    "    \n",
    "    return filtered_regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objekterkennung mit Region Proposals\n",
    "\n",
    "Diese Funktionen erkennen Objekte in einem Bild mit Region Proposals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_objects_with_region_proposals(image, model, object_type, confidence_threshold=0.7):\n",
    "    \"\"\"\n",
    "    Erkennt Objekte in einem Bild mit Region Proposals.\n",
    "    \n",
    "    Args:\n",
    "        image: Eingabebild\n",
    "        model: Trainiertes Modell\n",
    "        object_type: 'person' oder 'car'\n",
    "        confidence_threshold: Schwellenwert für die Konfidenz\n",
    "        \n",
    "    Returns:\n",
    "        detections: Liste der erkannten Objekte (x, y, w, h, confidence, object_type)\n",
    "    \"\"\"\n",
    "    # Regionen mit Region Proposals vorschlagen\n",
    "    regions = generate_region_proposals(image)\n",
    "    \n",
    "    detections = []\n",
    "    \n",
    "    for x, y, w, h in regions:\n",
    "        # Extrahieren der Region\n",
    "        if y + h <= image.shape[0] and x + w <= image.shape[1]:  # Sicherstellen, dass die Region innerhalb des Bildes liegt\n",
    "            region = image[y:y+h, x:x+w]\n",
    "            \n",
    "            # Vorverarbeitung der Region\n",
    "            try:\n",
    "                pil_region = Image.fromarray(region)\n",
    "                region_resized = np.array(pil_region.resize((32, 32)))\n",
    "                region_normalized = region_resized.astype('float32') / 255.0\n",
    "                region_batch = np.expand_dims(region_normalized, axis=0)\n",
    "                \n",
    "                # Vorhersage\n",
    "                prediction = model.predict(region_batch, verbose=0)[0][0]\n",
    "                \n",
    "                # Wenn die Konfidenz über dem Schwellenwert liegt, speichern wir die Erkennung\n",
    "                if prediction > confidence_threshold:\n",
    "                    detections.append((x, y, w, h, prediction, object_type))\n",
    "            except Exception as e:\n",
    "                # Ignorieren von Regionen, die nicht verarbeitet werden können\n",
    "                continue\n",
    "    \n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_objects_multi_scale(image, model, object_type, scales=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0, 2.5], \n",
    "                              confidence_threshold=0.7):\n",
    "    \"\"\"\n",
    "    Erkennt Objekte in einem Bild mit Multi-Scale-Erkennung.\n",
    "    \n",
    "    Args:\n",
    "        image: Eingabebild\n",
    "        model: Trainiertes Modell\n",
    "        object_type: 'person' oder 'car'\n",
    "        scales: Liste der Skalierungsfaktoren\n",
    "        confidence_threshold: Schwellenwert für die Konfidenz\n",
    "        \n",
    "    Returns:\n",
    "        detections: Liste der erkannten Objekte (x, y, w, h, confidence, object_type)\n",
    "    \"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    detections = []\n",
    "    \n",
    "    # Erkennung mit Region Proposals auf dem Originalbild\n",
    "    detections.extend(detect_objects_with_region_proposals(image, model, object_type, confidence_threshold))\n",
    "    \n",
    "    # Multi-Scale-Erkennung\n",
    "    for scale in scales:\n",
    "        # Skalieren des Bildes\n",
    "        scaled_height = int(height * scale)\n",
    "        scaled_width = int(width * scale)\n",
    "        \n",
    "        # Verwenden von PIL für die Skalierung\n",
    "        pil_image = Image.fromarray(image)\n",
    "        scaled_image = np.array(pil_image.resize((scaled_width, scaled_height)))\n",
    "        \n",
    "        # Erkennen von Objekten im skalierten Bild mit Region Proposals\n",
    "        scaled_detections = detect_objects_with_region_proposals(scaled_image, model, object_type, confidence_threshold)\n",
    "        \n",
    "        # Anpassen der Koordinaten an die Originalgröße\n",
    "        for (x, y, w, h, conf, obj_type) in scaled_detections:\n",
    "            x_orig = int(x / scale)\n",
    "            y_orig = int(y / scale)\n",
    "            w_orig = int(w / scale)\n",
    "            h_orig = int(h / scale)\n",
    "            detections.append((x_orig, y_orig, w_orig, h_orig, conf, obj_type))\n",
    "    \n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_large_objects(image, model, object_type, confidence_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Erkennt große Objekte, die einen signifikanten Teil des Bildes einnehmen.\n",
    "    \n",
    "    Args:\n",
    "        image: Eingabebild\n",
    "        model: Trainiertes Modell\n",
    "        object_type: 'person' oder 'car'\n",
    "        confidence_threshold: Schwellenwert für die Konfidenz\n",
    "        \n",
    "    Returns:\n",
    "        detections: Liste der erkannten großen Objekte (x, y, w, h, confidence, object_type)\n",
    "    \"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    detections = []\n",
    "    \n",
    "    # Definieren von großen Regionen, die signifikante Teile des Bildes abdecken\n",
    "    # Wir erstellen Regionen, die 40% bis 90% des Bildes abdecken\n",
    "    coverage_ratios = [0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    \n",
    "    for ratio in coverage_ratios:\n",
    "        # Berechnen der Größe der Region basierend auf dem Verhältnis\n",
    "        region_width = int(width * ratio)\n",
    "        region_height = int(height * ratio)\n",
    "        \n",
    "        # Berechnen der Position der Region (zentriert)\n",
    "        x = (width - region_width) // 2\n",
    "        y = (height - region_height) // 2\n",
    "        \n",
    "        # Extrahieren der Region\n",
    "        region = image[y:y+region_height, x:x+region_width]\n",
    "        \n",
    "        # Vorverarbeitung der Region\n",
    "        try:\n",
    "            pil_region = Image.fromarray(region)\n",
    "            region_resized = np.array(pil_region.resize((32, 32)))\n",
    "            region_normalized = region_resized.astype('float32') / 255.0\n",
    "            region_batch = np.expand_dims(region_normalized, axis=0)\n",
    "            \n",
    "            # Vorhersage\n",
    "            prediction = model.predict(region_batch, verbose=0)[0][0]\n",
    "            \n",
    "            # Wenn die Konfidenz über dem Schwellenwert liegt, speichern wir die Erkennung\n",
    "            if prediction > confidence_threshold:\n",
    "                detections.append((x, y, region_width, region_height, prediction, object_type))\n",
    "        except Exception as e:\n",
    "            # Ignorieren von Regionen, die nicht verarbeitet werden können\n",
    "            continue\n",
    "    \n",
    "    return detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erweiterte Non-Maximum Suppression\n",
    "\n",
    "Diese Funktion implementiert eine erweiterte Non-Maximum Suppression, die 100% überlappende Boxen entfernt und eine intelligente Filterung für verschachtelte Boxen bietet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_non_max_suppression(boxes, overlap_threshold=0.3, score_threshold=0.6, containment_threshold=0.95):\n",
    "    \"\"\"\n",
    "    Führt eine erweiterte Non-Maximum Suppression durch, die 100% überlappende Boxen entfernt\n",
    "    und eine intelligente Filterung für verschachtelte Boxen implementiert.\n",
    "    \n",
    "    Args:\n",
    "        boxes: Liste der Bounding Boxes (x, y, w, h, confidence, object_type)\n",
    "        overlap_threshold: Schwellenwert für die Überlappung (IoU)\n",
    "        score_threshold: Minimaler Konfidenzwert\n",
    "        containment_threshold: Schwellenwert für die Enthaltung einer Box in einer anderen\n",
    "        \n",
    "    Returns:\n",
    "        picked: Liste der ausgewählten Bounding Boxes\n",
    "    \"\"\"\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Filtern nach Konfidenz\n",
    "    boxes = [box for box in boxes if box[4] >= score_threshold]\n",
    "    \n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Gruppieren der Boxen nach Objekttyp\n",
    "    person_boxes = [box for box in boxes if box[5] == 'person']\n",
    "    car_boxes = [box for box in boxes if box[5] == 'car']\n",
    "    \n",
    "    # Verarbeiten der Boxen für jeden Objekttyp separat\n",
    "    picked_persons = process_boxes_by_type(person_boxes, overlap_threshold, containment_threshold)\n",
    "    picked_cars = process_boxes_by_type(car_boxes, overlap_threshold, containment_threshold)\n",
    "    \n",
    "    # Kombinieren der Ergebnisse\n",
    "    return picked_persons + picked_cars\n",
    "\n",
    "def process_boxes_by_type(boxes, overlap_threshold, containment_threshold):\n",
    "    \"\"\"\n",
    "    Verarbeitet Bounding Boxes eines bestimmten Typs mit Non-Maximum Suppression.\n",
    "    \n",
    "    Args:\n",
    "        boxes: Liste der Bounding Boxes (x, y, w, h, confidence, object_type)\n",
    "        overlap_threshold: Schwellenwert für die Überlappung (IoU)\n",
    "        containment_threshold: Schwellenwert für die Enthaltung einer Box in einer anderen\n",
    "        \n",
    "    Returns:\n",
    "        picked: Liste der ausgewählten Bounding Boxes\n",
    "    \"\"\"\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Konvertieren der Bounding Boxes in das Format (x1, y1, x2, y2, conf, object_type)\n",
    "    boxes_array = np.array([(x, y, x + w, y + h, conf, obj_type) for x, y, w, h, conf, obj_type in boxes])\n",
    "    \n",
    "    # Sortieren der Bounding Boxes nach Konfidenz (absteigend)\n",
    "    boxes_array = boxes_array[np.argsort(boxes_array[:, 4])[::-1]]\n",
    "    \n",
    "    # Entfernen von 100% überlappenden Boxen (Duplikate)\n",
    "    unique_boxes = []\n",
    "    for box in boxes_array:\n",
    "        is_duplicate = False\n",
    "        for unique_box in unique_boxes:\n",
    "            if (box[0] == unique_box[0] and box[1] == unique_box[1] and \n",
    "                box[2] == unique_box[2] and box[3] == unique_box[3]):\n",
    "                is_duplicate = True\n",
    "                break\n",
    "        if not is_duplicate:\n",
    "            unique_boxes.append(box)\n",
    "    \n",
    "    boxes_array = np.array(unique_boxes)\n",
    "    \n",
    "    picked = []\n",
    "    \n",
    "    while len(boxes_array) > 0:\n",
    "        # Die Box mit der höchsten Konfidenz auswählen\n",
    "        current_box = boxes_array[0]\n",
    "        picked.append(current_box)\n",
    "        \n",
    "        # Berechnen der Überlappung mit den verbleibenden Boxen\n",
    "        remaining_boxes = boxes_array[1:]\n",
    "        \n",
    "        if len(remaining_boxes) == 0:\n",
    "            break\n",
    "        \n",
    "        # Berechnen der Koordinaten der Überlappung\n",
    "        xx1 = np.maximum(current_box[0], remaining_boxes[:, 0])\n",
    "        yy1 = np.maximum(current_box[1], remaining_boxes[:, 1])\n",
    "        xx2 = np.minimum(current_box[2], remaining_boxes[:, 2])\n",
    "        yy2 = np.minimum(current_box[3], remaining_boxes[:, 3])\n",
    "        \n",
    "        # Berechnen der Breite und Höhe der Überlappung\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    "        \n",
    "        # Berechnen des Überlappungsverhältnisses (IoU - Intersection over Union)\n",
    "        intersection = w * h\n",
    "        area1 = (current_box[2] - current_box[0] + 1) * (current_box[3] - current_box[1] + 1)\n",
    "        area2 = (remaining_boxes[:, 2] - remaining_boxes[:, 0] + 1) * (remaining_boxes[:, 3] - remaining_boxes[:, 1] + 1)\n",
    "        union = area1 + area2 - intersection\n",
    "        iou = intersection / union\n",
    "        \n",
    "        # Berechnen des Verhältnisses der Überlappung zur Fläche der kleineren Box\n",
    "        # Dies hilft zu erkennen, ob eine Box fast vollständig in einer anderen enthalten ist\n",
    "        containment_ratio1 = intersection / area1  # Wie viel von Box 1 ist in der Überlappung enthalten\n",
    "        containment_ratio2 = intersection / area2  # Wie viel von Box 2 ist in der Überlappung enthalten\n",
    "        \n",
    "        # Indizes der Boxen, die entfernt werden sollen\n",
    "        to_remove = []\n",
    "        \n",
    "        for i in range(len(remaining_boxes)):\n",
    "            # Wenn die IoU über dem Schwellenwert liegt, entfernen wir die Box\n",
    "            if iou[i] > overlap_threshold:\n",
    "                to_remove.append(i)\n",
    "            # Wenn eine Box fast vollständig in der aktuellen Box enthalten ist\n",
    "            elif containment_ratio2[i] > containment_threshold:\n",
    "                # Wenn die kleinere Box eine höhere Konfidenz hat, behalten wir sie\n",
    "                if remaining_boxes[i, 4] > current_box[4] * 1.2:  # 20% höhere Konfidenz\n",
    "                    continue\n",
    "                to_remove.append(i)\n",
    "        \n",
    "        # Erstellen einer Maske für die zu behaltenden Boxen\n",
    "        mask = np.ones(len(remaining_boxes), dtype=bool)\n",
    "        mask[to_remove] = False\n",
    "        \n",
    "        # Aktualisieren der verbleibenden Boxen\n",
    "        boxes_array = remaining_boxes[mask]\n",
    "    \n",
    "    # Konvertieren zurück in das Format (x, y, w, h, confidence, object_type)\n",
    "    picked = [(box[0], box[1], box[2] - box[0], box[3] - box[1], box[4], box[5]) for box in picked]\n",
    "    \n",
    "    return picked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zeichnen der Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(image, boxes):\n",
    "    \"\"\"\n",
    "    Zeichnet Bounding Boxes auf ein Bild.\n",
    "    \n",
    "    Args:\n",
    "        image: Eingabebild\n",
    "        boxes: Liste der Bounding Boxes (x, y, w, h, confidence, object_type)\n",
    "        \n",
    "    Returns:\n",
    "        result: Bild mit Bounding Boxes\n",
    "    \"\"\"\n",
    "    # Convert numpy array to PIL Image\n",
    "    pil_image = Image.fromarray(image)\n",
    "    draw = ImageDraw.Draw(pil_image)\n",
    "    \n",
    "    for (x, y, w, h, conf, obj_type) in boxes:\n",
    "        # Convert coordinates to integers\n",
    "        x, y, w, h = int(x), int(y), int(w), int(h)\n",
    "        \n",
    "        # Farbe basierend auf Objekttyp\n",
    "        color = (0, 255, 0) if obj_type == 'car' else (255, 0, 0)  # Grün für Autos, Rot für Personen\n",
    "        \n",
    "        # Zeichnen der Bounding Box\n",
    "        draw.rectangle([(x, y), (x + w, y + h)], outline=color, width=2)\n",
    "        \n",
    "        # Zeichnen des Labels\n",
    "        label = f\"{obj_type.capitalize()}: {conf:.2f}\"\n",
    "        draw.text((x, y - 10), label, fill=color)\n",
    "    \n",
    "    # Convert back to numpy array\n",
    "    result = np.array(pil_image)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hauptfunktion zur Erkennung von Personen und Autos in Bildern\n",
    "\n",
    "Diese Funktion kombiniert alle vorherigen Funktionen, um Personen und Autos in einem Bild zu erkennen und zu markieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_draw_objects(image_path, person_model, car_model, output_path):\n",
    "    \"\"\"\n",
    "    Erkennt Personen und Autos in einem Bild und zeichnet Bounding Boxes.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Pfad zum Bild oder URL\n",
    "        person_model: Trainiertes Modell für Personenerkennung\n",
    "        car_model: Trainiertes Modell für Autoerkennung\n",
    "        output_path: Pfad zum Ausgabebild\n",
    "        \n",
    "    Returns:\n",
    "        boxes: Liste der erkannten Objekte (x, y, w, h, confidence, object_type)\n",
    "    \"\"\"\n",
    "    # Laden und Vorverarbeiten des Bildes\n",
    "    image, processed_image, (original_height, original_width) = load_and_preprocess_image(image_path)\n",
    "    \n",
    "    # Zeitmessung starten\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Erkennen von Personen im Bild\n",
    "    person_boxes = detect_objects_multi_scale(image, person_model, 'person', confidence_threshold=0.6)\n",
    "    \n",
    "    # Erkennen von großen Personen\n",
    "    large_person_boxes = detect_large_objects(image, person_model, 'person', confidence_threshold=0.6)\n",
    "    person_boxes.extend(large_person_boxes)\n",
    "    \n",
    "    # Erkennen von Autos im Bild\n",
    "    car_boxes = detect_objects_multi_scale(image, car_model, 'car', confidence_threshold=0.6)\n",
    "    \n",
    "    # Erkennen von großen Autos\n",
    "    large_car_boxes = detect_large_objects(image, car_model, 'car', confidence_threshold=0.6)\n",
    "    car_boxes.extend(large_car_boxes)\n",
    "    \n",
    "    # Kombinieren der Erkennungen\n",
    "    all_boxes = person_boxes + car_boxes\n",
    "    \n",
    "    # Zusammenführen überlappender Bounding Boxes mit erweiterter NMS\n",
    "    boxes = advanced_non_max_suppression(all_boxes, overlap_threshold=0.3, score_threshold=0.6, containment_threshold=0.95)\n",
    "    \n",
    "    # Zeitmessung beenden\n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    \n",
    "    # Zeichnen der Bounding Boxes\n",
    "    result = draw_boxes(image, boxes)\n",
    "    \n",
    "    # Hinzufügen von Informationen zum Bild\n",
    "    pil_result = Image.fromarray(result)\n",
    "    draw = ImageDraw.Draw(pil_result)\n",
    "    person_count = sum(1 for box in boxes if box[5] == 'person')\n",
    "    car_count = sum(1 for box in boxes if box[5] == 'car')\n",
    "    info_text = f\"Erkannte Personen: {person_count} | Erkannte Autos: {car_count} | Verarbeitungszeit: {processing_time:.2f}s\"\n",
    "    draw.text((10, 30), info_text, fill=(0, 0, 255))\n",
    "    result = np.array(pil_result)\n",
    "    \n",
    "    # Speichern des Ergebnisses\n",
    "    Image.fromarray(result).save(output_path)\n",
    "    \n",
    "    # Erstellen einzelner Bilder für jedes erkannte Objekt\n",
    "    for i, (x, y, w, h, conf, obj_type) in enumerate(boxes):\n",
    "        # Convert coordinates to integers\n",
    "        x, y, w, h = int(x), int(y), int(w), int(h)\n",
    "        \n",
    "        # Extrahieren des Objekts\n",
    "        object_image = image[y:y+h, x:x+w]\n",
    "        \n",
    "        # Zeichnen einer Box um das Objekt\n",
    "        pil_object = Image.fromarray(object_image)\n",
    "        draw = ImageDraw.Draw(pil_object)\n",
    "        color = (0, 255, 0) if obj_type == 'car' else (255, 0, 0)  # Grün für Autos, Rot für Personen\n",
    "        draw.rectangle([(0, 0), (w, h)], outline=color, width=2)\n",
    "        object_image_with_box = np.array(pil_object)\n",
    "        \n",
    "        # Speichern des Bildes\n",
    "        object_output_path = output_path.replace('.jpg', f'_{obj_type}_{i+1}.jpg')\n",
    "        Image.fromarray(object_image_with_box).save(object_output_path)\n",
    "    \n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anwendung auf Testbilder\n",
    "\n",
    "### Testen der Personen- und Autoerkennung auf Bildern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testen der Personen- und Autoerkennung auf Bildern...\")\n",
    "\n",
    "# Testbilder mit Personen und Autos\n",
    "test_images = [\n",
    "    \"https://cdn.pixabay.com/photo/2017/08/06/15/13/people-2593378_1280.jpg\",  # Personen auf der Straße\n",
    "    \"https://cdn.pixabay.com/photo/2016/11/18/12/14/car-1834274_1280.jpg\",     # Auto mit Person\n",
    "    \"https://cdn.pixabay.com/photo/2017/08/01/11/48/woman-2564660_1280.jpg\"    # Person neben Auto\n",
    "]\n",
    "\n",
    "for i, image_url in enumerate(test_images):\n",
    "    # Speichern des Bildes\n",
    "    image_path = os.path.join(images_dir, f'test_image_{i+1}.jpg')\n",
    "    \n",
    "    # Herunterladen des Bildes, wenn es noch nicht existiert\n",
    "    if not os.path.exists(image_path):\n",
    "        response = requests.get(image_url)\n",
    "        with open(image_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "    \n",
    "    # Erkennen von Personen und Autos im Bild\n",
    "    output_path = os.path.join(bonus_dir, f'test_image_{i+1}_result.jpg')\n",
    "    boxes = detect_and_draw_objects(image_path, person_model, car_model, output_path)\n",
    "    \n",
    "    # Zählen der erkannten Objekte\n",
    "    person_count = sum(1 for box in boxes if box[5] == 'person')\n",
    "    car_count = sum(1 for box in boxes if box[5] == 'car')\n",
    "    \n",
    "    print(f\"Bild {i+1}: {person_count} Personen und {car_count} Autos erkannt\")\n",
    "    \n",
    "    # Anzeigen des Ergebnisses im Notebook\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(plt.imread(output_path))\n",
    "    plt.title(f\"Bild {i+1}: {person_count} Personen und {car_count} Autos erkannt\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testen der Personen- und Autoerkennung auf den Bildern aus dem Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testen der Personen- und Autoerkennung auf den Bildern aus dem Repository...\")\n",
    "\n",
    "# Bilder aus dem Repository\n",
    "repo_images = glob.glob(os.path.join(images_dir, 'bild*.jpg'))\n",
    "\n",
    "for i, image_path in enumerate(repo_images):\n",
    "    # Erkennen von Personen und Autos im Bild\n",
    "    output_path = os.path.join(bonus_dir, f'repo_image_{i+1}_result.jpg')\n",
    "    boxes = detect_and_draw_objects(image_path, person_model, car_model, output_path)\n",
    "    \n",
    "    # Zählen der erkannten Objekte\n",
    "    person_count = sum(1 for box in boxes if box[5] == 'person')\n",
    "    car_count = sum(1 for box in boxes if box[5] == 'car')\n",
    "    \n",
    "    print(f\"Repository-Bild {i+1}: {person_count} Personen und {car_count} Autos erkannt\")\n",
    "    \n",
    "    # Anzeigen des Ergebnisses im Notebook\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(plt.imread(output_path))\n",
    "    plt.title(f\"Repository-Bild {i+1}: {person_count} Personen und {car_count} Autos erkannt\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zusammenfassung\n",
    "\n",
    "In diesem Notebook haben wir:\n",
    "1. Einen Datensatz für Personenerkennung aus dem HumanBinaryClassificationSuite Repository geladen und vorbereitet\n",
    "2. Ein CNN-Modell für Personenerkennung definiert und trainiert\n",
    "3. Das trainierte Personenerkennungsmodell und das vorhandene Autoerkennungsmodell geladen\n",
    "4. Eine verbesserte Erkennungsmethode basierend auf dem 05-Notebook implementiert, die Region Proposals und Multi-Scale-Erkennung verwendet\n",
    "5. Eine spezielle Erkennung für große Objekte implementiert, die einen signifikanten Teil des Bildes einnehmen\n",
    "6. Eine erweiterte Non-Maximum Suppression implementiert, die 100% überlappende Boxen entfernt und eine intelligente Filterung für verschachtelte Boxen bietet\n",
    "7. Die Erkennung auf verschiedenen Testbildern angewendet und die Ergebnisse visualisiert\n",
    "\n",
    "Der implementierte Ansatz ermöglicht eine zuverlässige Erkennung von Personen und Autos in Bildern unterschiedlicher Größen und Perspektiven. Die Verwendung von Region Proposals anstelle des Sliding-Window-Ansatzes führt zu einer besseren Erkennung, insbesondere in komplexen Szenen mit mehreren Objekten. Die spezielle Erkennung für große Objekte ermöglicht es, Personen und Autos zu erkennen, die einen signifikanten Teil des Bildes einnehmen. Die erweiterte Non-Maximum Suppression verhindert 100% überlappende Boxen und bietet eine intelligente Filterung für verschachtelte Boxen, die nicht einfach kleinere Boxen innerhalb größerer Boxen entfernt, sondern die Konfidenz und das Enthaltensein berücksichtigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Personen- und Autoerkennung abgeschlossen. Die Ergebnisse wurden im Verzeichnis 'bonus' gespeichert.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
